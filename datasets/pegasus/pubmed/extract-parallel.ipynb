{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed Extraction\n",
    "\n",
    "Extract desired data from pubmed XML files into csv files to be used in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as etree\n",
    "from pathlib import Path\n",
    "import gzip, csv\n",
    "from multiprocessing import Lock\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data/sample\") # you probably want a symlink here to your data drive\n",
    "dest = Path(\".\")      # for faster speed if not SSD-drive, set dest to a different drive\n",
    "#dest = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once to download the pubmed data \n",
    "# info: 1200+ xml.gz files totalling ~35GB compressed (2019-05)\n",
    "if 0:\n",
    "    ! wget -m -np -nd ftp://ftp.ncbi.nlm.nih.gov/pubmed/baseline/    -P {path}\n",
    "    ! wget -m -np -nd ftp://ftp.ncbi.nlm.nih.gov/pubmed/updatefiles/ -P {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/sample/pubmed19n1198.xml.gz'),\n",
       " PosixPath('data/sample/pubmed19n1157.xml.gz'),\n",
       " PosixPath('data/sample/pubmed19n1185.xml.gz'),\n",
       " PosixPath('data/sample/pubmed19n1152.xml.gz'),\n",
       " PosixPath('data/sample/pubmed19n1205.xml.gz'),\n",
       " PosixPath('data/sample/pubmed19n1172.xml.gz')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_thread(func, arr:Collection, max_workers:int=None):\n",
    "    \"Call `func` on every element of `arr` in parallel using `max_workers`.\"\n",
    "    max_workers = ifnone(max_workers, defaults.cpus)\n",
    "    if max_workers<2: results = [func(o,i) for i,o in progress_bar(enumerate(arr), total=len(arr))]\n",
    "    else:\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "            futures = [ex.submit(func,o,i) for i,o in enumerate(arr)]\n",
    "            results = []\n",
    "            for f in progress_bar(concurrent.futures.as_completed(futures), total=len(arr)): results.append(f.result())\n",
    "    if any([o is not None for o in results]): return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files to process: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 00:04<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000 data/sample/pubmed19n1152.xml.gz: recs 01637, eng 01627, no abstract 00257\n",
      "0005 data/sample/pubmed19n1205.xml.gz: recs 02848, eng 02845, no abstract 00442\n",
      "0002 data/sample/pubmed19n1172.xml.gz: recs 03217, eng 03161, no abstract 00401\n",
      "0004 data/sample/pubmed19n1198.xml.gz: recs 04936, eng 04612, no abstract 00296\n",
      "0001 data/sample/pubmed19n1157.xml.gz: recs 05062, eng 05040, no abstract 00338\n",
      "0003 data/sample/pubmed19n1185.xml.gz: recs 06561, eng 06433, no abstract 00693\n",
      "Grand total of 0 English recs (no abstract 0) out of 0 pubmed recs\n",
      "Saving MeSh records (0)\n",
      "   24366 pubmed-abstracts.csv\n",
      "       1 pubmed-mesh.csv\n",
      "   24367 total\n",
      "CPU times: user 11.3 ms, sys: 61.6 ms, total: 72.8 ms\n",
      "Wall time: 4.89 s\n"
     ]
    }
   ],
   "source": [
    "# Data to extract:\n",
    "\n",
    "# Root: PubmedArticleSet.PubmedArticle.MedlineCitation\n",
    "\n",
    "#  Condition: # only english articles\n",
    "# -- .Language == eng\n",
    "\n",
    "# 1. pmid | title | abstract | mesh (pubmed-abstracts.csv)\n",
    "# - .PMID\n",
    "# - .Article:\n",
    "# -- .ArticleTitle\n",
    "# -- .Abstract.AbstractText\n",
    "#\n",
    "# 2.  MeSH data \n",
    "# - .MeshHeadingList: converted into a single record with '|' separated entries of each MeshHeading\n",
    "#    and each MeshHeading is converted into 'UI/MajorTopicYN' for DescriptorName, \n",
    "#    and optional 'UI/MajorTopicYN' for QualifierName joined with '-' \n",
    "#    here is a sample of what a record might look like: D000339/Y|D004650/N-Q000706/Y \n",
    "# \n",
    "#   the lookup table for these ids is saved in a separate pubmed-mesh.csv file\n",
    "\n",
    "csv_fn_main = dest/\"pubmed-abstracts.csv\"\n",
    "csv_fn_mesh = dest/\"pubmed-mesh.csv\"\n",
    "\n",
    "#lock = threading.Lock()\n",
    "total, total_eng, total_no_abstract = 0, 0, 0\n",
    "mesh_db = {}\n",
    "f_out_main = open(csv_fn_main, \"w\")\n",
    "csv_writer_main = csv.writer(f_out_main)\n",
    "csv_writer_main.writerow([\"pmid\", \"title\", \"abstract\", \"mesh\"])\n",
    "\n",
    "lock = Lock()\n",
    "\n",
    "def mesh_db_update(k, v):\n",
    "    if k in mesh_db: return\n",
    "    mesh_db[k]=v\n",
    "    \n",
    "def mesh_db_write():\n",
    "    print(f\"Saving MeSh records ({len(mesh_db)})\")\n",
    "    with open(csv_fn_mesh, \"w\") as f_out_mesh:\n",
    "        csv_writer_mesh = csv.writer(f_out_mesh)\n",
    "        csv_writer_mesh.writerow([\"meshid\", \"text\"])\n",
    "        csv_writer_mesh.writerows(mesh_db.items())\n",
    "\n",
    "# https://stackoverflow.com/a/26435241/9201239 efficient RAM usage\n",
    "def parse_entries(f, tag):\n",
    "    \"\"\"Yield *tag* elements from *f* xml (fn or fh) incrementaly.\"\"\"\n",
    "    context = iter(etree.iterparse(f, events=('start', 'end')))\n",
    "    _, root = next(context) # get root element\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag == tag:\n",
    "            yield elem\n",
    "            root.clear() # free memory\n",
    "\n",
    "def meshhead2rec(mh):\n",
    "    if mh is None: return ''\n",
    "    l = []\n",
    "    for e in mh.find('DescriptorName'), mh.find('QualifierName'):\n",
    "        if e is not None:\n",
    "            ui, mt = e.attrib['UI'], e.attrib['MajorTopicYN']\n",
    "            l.append(f\"{ui}/{mt}\")\n",
    "            mesh_db_update(ui, e.text)\n",
    "    return \"-\".join(l)\n",
    "                \n",
    "def meshlist2rec(mhl):\n",
    "    if mhl is None: return ''\n",
    "    l = [meshhead2rec(m) for m in mhl.findall('MeshHeading')]\n",
    "    return \"|\".join(l)\n",
    "\n",
    "#def extract(f_in, fn, files_cnt, csv_writer_main):\n",
    "def extract(f_in, fn, files_cnt):\n",
    "    global total, total_eng, total_no_abstract\n",
    "    #print(total, total_eng, total_no_abstract)\n",
    "    c, c_eng, c_no_abstract = 0, 0, 0\n",
    "    rows = []\n",
    "    for e in parse_entries(f_in, 'MedlineCitation'):\n",
    "        c += 1\n",
    "        try:\n",
    "            pmid = e.find('PMID').text\n",
    "            #print(pmid)\n",
    "\n",
    "            # 1. Abstracts\n",
    "            a = e.find('Article')\n",
    "            if a is None: continue\n",
    "                \n",
    "            lang = a.find('Language')\n",
    "            if lang is None or lang.text != 'eng': continue\n",
    "\n",
    "            c_eng += 1\n",
    "            title    = a.find('ArticleTitle').text\n",
    "            abstract = a.find('Abstract')\n",
    "            if abstract is not None:\n",
    "                abstract_text = abstract.find('AbstractText').text\n",
    "                #print(pmid, title, abstract)\n",
    "            else:\n",
    "                abstract_text = ''\n",
    "                c_no_abstract += 1\n",
    "\n",
    "            \n",
    "            # 2. MeSH Data\n",
    "            mesh = meshlist2rec(e.find('MeshHeadingList'))\n",
    "            #print(f\"MeSH: {mesh}\")\n",
    "            \n",
    "            # \n",
    "            #with lock:\n",
    "            #    csv_writer_main.writerow([pmid, title, abstract_text, mesh])\n",
    "            rows.append([pmid, title, abstract_text, mesh])\n",
    "                    \n",
    "        except: \n",
    "            #if not pmid: pmid = \"unknown\"\n",
    "            #print(f\"{pmid} failed to parse\")\n",
    "            raise\n",
    "        #break\n",
    "        \n",
    "    print(f\"{files_cnt:0>4d} {fn}: recs {c:0>5d}, eng {c_eng:0>5d}, no abstract {c_no_abstract:0>5d}\")\n",
    "    with lock:\n",
    "        csv_writer_main.writerows(rows)\n",
    "        mesh_db_write()\n",
    "\n",
    "def extract_parallel(fn, index):\n",
    "    #print(f\"job {index}: file: {fn}\")\n",
    "    with gzip.open(fn, 'rb') as f_in: extract(f_in, fn, index)\n",
    "        \n",
    "def process():\n",
    "\n",
    "    files = sorted(path.glob(\"*.xml.gz\"))\n",
    "    print(f\"Total files to process: {len(files)}\")\n",
    "    parallel(extract_parallel, files, max_workers=8)           \n",
    "\n",
    "    # XXX: mesh writing is broken in multiproc - need to save each separately and then merge at the end of everything, as simple as `cat mesh*csv | uniq > mesh.csv` hack will do\n",
    "    \n",
    "    # summary\n",
    "    ! wc -l {csv_fn_main} {csv_fn_mesh}\n",
    "    \n",
    "%time process()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
