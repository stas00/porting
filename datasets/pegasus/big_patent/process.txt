### big_patent ###

# 0. setup

pip install pegasus

# nltk and data
python -c "import nltk; nltk.download()"

# tfds expects `ntlk_data` under the python package  (.../lib/python3.8/site-packages/nltk/)
# if your `nltk_data` is not under nltk package, like mine which is under ~/nltk_data, you can either set:
export NLTK_DATA="~/nltk_data"
# or symlink (adjust the path)
ln -s ~/nltk_data /home/stas/anaconda3/envs/main-38/lib/python3.8/site-packages/nltk/


# 1. then manually build the dataset (from: https://www.tensorflow.org/datasets/beam_datasets#locally)
# this is going to take a really really long time - it will use just one CPU.

/usr/bin/nice -n 19 /usr/bin/ionice -n 0 python -m tensorflow_datasets.scripts.download_and_prepare --datasets=big_patent/all

# 2. run

./process.py

# generated files to be fed into eval are ./data/test.*

# 3. short eval

python -m torch.distributed.launch --nproc_per_node=2 run_distributed_eval.py --model_name google/pegasus-big_patent --save_dir xsum_generations --data_dir /hf/pegasus-datasets/bigpatent/data --prefix test --n_obs 100 --bs 4 --min_length 32

# 4. full eval

python -m torch.distributed.launch --nproc_per_node=2 run_distributed_eval.py --model_name google/pegasus-big_patent --save_dir xsum_generations --data_dir /hf/pegasus-datasets/bigpatent/data --prefix test --bs 4 --min_length 32

# 5. final data to upload
cd ..
tar -cvzf big_patent-test.tgz bigpatent/data/test.*



