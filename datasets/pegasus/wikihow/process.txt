### wikihow ###

# 0. setup

pip install datasets

# 1. manually download https://ucsb.app.box.com/s/ap23l8gafpezf4tq3wapr6u8241zz358 and save to some path

# 2. adjust data_dir below to where it was downloaded

# 3. run

./process.py

# generated files to be fed into eval are ./data/test.*

# 4. short eval

python -m torch.distributed.launch --nproc_per_node=2 run_distributed_eval.py --model_name google/pegasus-wikihow --save_dir xsum_generations --data_dir /hf/pegasus-datasets/wikihow/data --prefix test --n_obs 100 --bs 4 --min_length 32


# 5. full eval

python -m torch.distributed.launch --nproc_per_node=2 run_distributed_eval.py --model_name google/pegasus-wikihow --save_dir xsum_generations --data_dir /hf/pegasus-datasets/wikihow/data --prefix test --bs 4 --min_length 32


# 6. final data to upload
cd ..
tar -cvzf wikihow-test.tgz wikihow/data/test.*










# this is old approach - discarded to better match the authors eval

# 0. setup
mkdir -p /hf/pegasus-datasets/wikihow/
cd /hf/pegasus-datasets/wikihow/

# 1. get data: @valhalla's source
gdown -O train_articles.zip --id 1-1CR6jh6StaI69AsbBXD8lQskFbGc2Ez # train
gdown -O valid_articles_.zip --id 1-EGoT5ZKRNHQb_ewNpD9GZCvQ3uHzDSi # val
gdown -O test_articles_.zip --id 1-CxzdzEIuBYzCs06zrglYrLBlLI6kjSZ # test 
unzip test_articles_.zip
unzip train_articles.zip
unzip valid_articles_.zip

# 2. pre-process
./process.py

# 3. short eval (reference point)
python -m torch.distributed.launch --nproc_per_node=2 run_distributed_eval.py --model_name google/pegasus-wikihow --save_dir xsum_generations --data_dir /hf/pegasus-datasets/wikihow/data --prefix test --n_obs 100 --bs 4 --min_length 32

# 4. full eval
python -m torch.distributed.launch --nproc_per_node=2 run_distributed_eval.py --model_name google/pegasus-wikihow --save_dir xsum_generations --data_dir /hf/pegasus-datasets/wikihow/data --prefix test --bs 4 --min_length 32


# extra args to experiment with:
--min_length 32
--length_penalty 1.0
