{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of DeepSpeed on colab CLI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsT5mHM6VTpt"
      },
      "source": [
        "\n",
        "◄ [**Open in Colab**](https://colab.research.google.com/github/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb) ► \n",
        "\n",
        "**Last modified: Mon 21 Mar 2022 03:24:37 PM PDT**\n",
        "\n",
        "# transformers + deepspeed CLI\n",
        "\n",
        "This notebook demonstrates how to setup `transformers` + `deepspeed` on colab to be run as an external process.\n",
        "\n",
        "You can of course use it under any notebook environment.\n",
        "\n",
        "It's possible to run `transformers` + `deepspeed` inside the notebook as well: \n",
        "\n",
        "**XXX**: make another notebook with a demo that isn't CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6S7Z35-TkSR"
      },
      "source": [
        "\n",
        "\n",
        "## Setting up the correct environment\n",
        "\n",
        "In order to run `transformers` with `deepspeed`, you need:\n",
        "1. enough general RAM. Different users seem to get a instance with different size of allocated general RAM. Try `!free -h` and if your process gets killed, you probably run out of memory. If you can't get enough memory you can turn `cpu_offload` off in `ds_config.json` below.\n",
        "2. matching cuda versions. Your pytorch needs to be built with the exact cuda version as you system-wide installed cuda. This is normally not needed to run `pytorch` alone, but is needed for building CUDA extensions, like DeepSpeed. You will find full documentation [here](https://huggingface.co/transformers/main_classes/trainer.html#installation-notes).\n",
        "\n",
        "Since we can't control which cuda version colab has it can be tricky to find the right matching pytorch version. So this notebook will save you time by already showing you all the required versions you need to install.\n",
        "\n",
        "Surely, this notebook will get outdated in time. So make sure you check for the latest version of it at https://github.com/stas00/porting/blob/master/transformers/deepspeed/ and please let me know if it needs to be updated if deepspeed stops building.\n",
        "\n",
        "As I mentioned earlier if Deepspeed builds but the training gets killed you got a colab instance with too little RAM. There is no need to contact me then as there is nothing I can do about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr8bCfITLOQe",
        "outputId": "38daca51-3cb3-4ef0-a93c-32105c3222d2"
      },
      "source": [
        "# Free colab seems to give different amount of general RAM to different users or even the same users at different times.\n",
        "\n",
        "!free -h"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        639M         10G        1.2M        1.9G         11G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ1iecs6SWgk",
        "outputId": "3ef82fb9-3c65-42b0-b59a-bcb3a8ea0ae9"
      },
      "source": [
        "# check which nvidia drivers and cuda version is running\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar 21 23:52:36 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P8    35W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9mmhJzcgHy1",
        "outputId": "812217cd-b5ac-4fc7-8b4c-9b5dff2aa3af"
      },
      "source": [
        "# need to match the system-wide installed cuda-11 for deepspeed to compile\n",
        "# so install the matching pytorch\n",
        "\n",
        "# pt-1.8.1 works too\n",
        "# !pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# pt-1.11\n",
        "!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[K     |████████████████▎               | 834.1 MB 1.4 MB/s eta 0:09:36tcmalloc: large alloc 1147494400 bytes == 0x55ea5fb0c000 @  0x7fa938ef1615 0x55ea272e03bc 0x55ea273c118a 0x55ea272e31cd 0x55ea273d5b3d 0x55ea27357458 0x55ea2735202f 0x55ea272e4aba 0x55ea273572c0 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea273d6986 0x55ea27353350 0x55ea273d6986 0x55ea27353350 0x55ea273d6986 0x55ea27353350 0x55ea272e4f19 0x55ea27328a79 0x55ea272e3b32 0x55ea273571dd 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f 0x55ea272e4aba 0x55ea27352eae 0x55ea272e49da 0x55ea27353108 0x55ea2735202f\n",
            "\u001b[K     |████████████████████▋           | 1055.7 MB 1.5 MB/s eta 0:06:29tcmalloc: large alloc 1434370048 bytes == 0x55eaa4162000 @  0x7fa938ef1615 0x55ea272e03bc 0x55ea273c118a 0x55ea272e31cd 0x55ea273d5b3d 0x55ea27357458 0x55ea2735202f 0x55ea272e4aba 0x55ea273572c0 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea273d6986 0x55ea27353350 0x55ea273d6986 0x55ea27353350 0x55ea273d6986 0x55ea27353350 0x55ea272e4f19 0x55ea27328a79 0x55ea272e3b32 0x55ea273571dd 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f 0x55ea272e4aba 0x55ea27352eae 0x55ea272e49da 0x55ea27353108 0x55ea2735202f\n",
            "\u001b[K     |██████████████████████████▏     | 1336.2 MB 1.4 MB/s eta 0:03:30tcmalloc: large alloc 1792966656 bytes == 0x55ea28f94000 @  0x7fa938ef1615 0x55ea272e03bc 0x55ea273c118a 0x55ea272e31cd 0x55ea273d5b3d 0x55ea27357458 0x55ea2735202f 0x55ea272e4aba 0x55ea273572c0 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea273d6986 0x55ea27353350 0x55ea273d6986 0x55ea27353350 0x55ea273d6986 0x55ea27353350 0x55ea272e4f19 0x55ea27328a79 0x55ea272e3b32 0x55ea273571dd 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f 0x55ea272e4aba 0x55ea27352eae 0x55ea272e49da 0x55ea27353108 0x55ea2735202f\n",
            "\u001b[K     |████████████████████████████████| 1636.9 MB 1.3 MB/s eta 0:00:01tcmalloc: large alloc 1636958208 bytes == 0x55ea93d7c000 @  0x7fa938ef01e7 0x55ea273165d7 0x55ea272e03bc 0x55ea273c118a 0x55ea272e31cd 0x55ea273d5b3d 0x55ea27357458 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea272e49da 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f\n",
            "tcmalloc: large alloc 2046197760 bytes == 0x55eaf569c000 @  0x7fa938ef1615 0x55ea272e03bc 0x55ea273c118a 0x55ea272e31cd 0x55ea273d5b3d 0x55ea27357458 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353108 0x55ea272e49da 0x55ea27353108 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f 0x55ea272e4aba 0x55ea27353cd4 0x55ea2735202f 0x55ea272e5151\n",
            "\u001b[K     |████████████████████████████████| 1637.0 MB 6.8 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 11.2 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp37-cp37m-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.11.0+cu113) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.12.0+cu113) (1.21.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision==0.12.0+cu113) (2021.10.8)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aNVOVxab2Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4713fe80-703f-483b-f660-b86ec79ce4a8"
      },
      "source": [
        "# either install the release\n",
        "#!pip install deepspeed\n",
        "# or the master \n",
        "!pip install git+https://github.com/microsoft/deepspeed\n",
        "\n",
        "# remove any previously cached deepspeed objects as they can be incompatible with this new build\n",
        "#!rm -r /root/.cache/torch_extensions/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/microsoft/deepspeed\n",
            "  Cloning https://github.com/microsoft/deepspeed to /tmp/pip-req-build-7qxh_7z8\n",
            "  Running command git clone -q https://github.com/microsoft/deepspeed /tmp/pip-req-build-7qxh_7z8\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Collecting hjson\n",
            "  Downloading hjson-3.0.2-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.6.1+0eb2c763) (1.21.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.6.1+0eb2c763) (21.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.6.1+0eb2c763) (5.4.8)\n",
            "Collecting py-cpuinfo\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.6.1+0eb2c763) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.6.1+0eb2c763) (4.63.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed==0.6.1+0eb2c763) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->deepspeed==0.6.1+0eb2c763) (3.10.0.2)\n",
            "Building wheels for collected packages: deepspeed, py-cpuinfo\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.6.1+0eb2c763-py3-none-any.whl size=573694 sha256=2b1383290e536a45d17fc92507aec6e6ae60edeae5712a420bd95718ef595e87\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8mzh279s/wheels/86/64/ea/dd1f2f3640c6a8733da7e93d63cce65df1d171095b92a98b5f\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=5abd74c61d2581257666cfac809f49e0ce92bdac8503c45dd110ac3a651e726d\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "Successfully built deepspeed py-cpuinfo\n",
            "Installing collected packages: py-cpuinfo, ninja, hjson, deepspeed\n",
            "Successfully installed deepspeed-0.6.1+0eb2c763 hjson-3.0.2 ninja-1.10.2.3 py-cpuinfo-8.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZQAIH70Yykn",
        "outputId": "6f81bb9d-bdfd-4297-a8d6-de0416f72106"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "# examples change a lot so let's pick a sha that we know this notebook will work with\n",
        "# comment out/remove the next line if you want the master\n",
        "git checkout 0aac9ba2dabcf9\n",
        "pip install -e .\n",
        "pip install -r examples/pytorch/translation/requirements.txt\n",
        "\n",
        "# if needed free up some space used by cached pip packages\n",
        "# rm -rf /root/.cache/pip\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.11.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (4.63.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (3.6.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.18.0.dev0) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.18.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.18.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.18.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.18.0.dev0) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.11.6 transformers\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.6.1-py3-none-any.whl (65 kB)\n",
            "Collecting datasets>=1.8.0\n",
            "  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 4)) (3.17.3)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "Collecting py7zr\n",
            "  Downloading py7zr-0.18.1-py3-none-any.whl (69 kB)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 7)) (1.11.0+cu113)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (4.63.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.21.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (0.3.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (0.4.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (4.11.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (21.3)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5)) (2019.12.20)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 5)) (0.8.9)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 7)) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r examples/pytorch/translation/requirements.txt (line 4)) (1.15.0)\n",
            "Collecting texttable\n",
            "  Downloading texttable-1.6.4-py2.py3-none-any.whl (10 kB)\n",
            "Collecting multivolumefile>=0.2.3\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting pyzstd>=0.14.4\n",
            "  Downloading pyzstd-0.15.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "Collecting pyppmd<0.18.0,>=0.17.0\n",
            "  Downloading pyppmd-0.17.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
            "Collecting brotli>=1.0.9\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "Collecting pybcj>=0.5.0\n",
            "  Downloading pybcj-0.5.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48 kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Downloading pycryptodomex-3.14.1-cp35-abi3-manylinux2010_x86_64.whl (2.0 MB)\n",
            "Collecting zipfile-deflate64>=0.2.0\n",
            "  Downloading zipfile_deflate64-0.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.0.12)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.8.0->-r examples/pytorch/translation/requirements.txt (line 2)) (2.8.2)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, zipfile-deflate64, xxhash, texttable, responses, pyzstd, pyppmd, pycryptodomex, pybcj, portalocker, multivolumefile, colorama, brotli, sentencepiece, sacrebleu, py7zr, datasets, accelerate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed accelerate-0.6.1 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 brotli-1.0.9 colorama-0.4.4 datasets-2.0.0 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 multivolumefile-0.2.3 portalocker-2.4.0 py7zr-0.18.1 pybcj-0.5.0 pycryptodomex-3.14.1 pyppmd-0.17.4 pyzstd-0.15.2 responses-0.18.0 sacrebleu-2.0.0 sentencepiece-0.1.96 texttable-1.6.4 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2 zipfile-deflate64-0.2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'transformers'...\n",
            "Note: checking out '0aac9ba2dabcf9'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 0aac9ba2d Add Flaubert OnnxConfig to Transformers (#16279)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYG2EDAQdFxt"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd transformers\n",
        "\n",
        "cat <<'EOT' > ds_config.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"allgather_partitions\": true,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": true,\n",
        "        \"reduce_scatter\": true,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJKdum6zdxLE"
      },
      "source": [
        "#!ls -l transformers\n",
        "#!cat transformers/ds_config.json"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XJEYx1sVuAJ"
      },
      "source": [
        "## Running Traning + Evaluation CLI style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghNfx0nNZSfq",
        "outputId": "77e7cd20-4f9c-4007-bf0c-aa27ca7dc8c7"
      },
      "source": [
        "!cd transformers; export BS=16; rm -rf output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps \\\n",
        "--do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 \\\n",
        "--max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir  \\\n",
        "--per_device_train_batch_size $BS --per_device_eval_batch_size $BS --predict_with_generate --sortish_sampler \\\n",
        "--val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_eval_samples 500 \\\n",
        "--dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro \\\n",
        "--source_prefix \"translate English to Romanian: \" --deepspeed ds_config.json --fp16"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-03-21 23:57:03,766] [WARNING] [runner.py:155:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "Detected CUDA_VISIBLE_DEVICES=0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.\n",
            "[2022-03-21 23:57:03,793] [INFO] [runner.py:453:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps --do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --predict_with_generate --sortish_sampler --val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_eval_samples 500 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro --source_prefix translate English to Romanian:  --deepspeed ds_config.json --fp16\n",
            "[2022-03-21 23:57:05,293] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.7.8\n",
            "[2022-03-21 23:57:05,294] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2022-03-21 23:57:05,294] [INFO] [launch.py:110:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2022-03-21 23:57:05,294] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2022-03-21 23:57:05,294] [INFO] [launch.py:123:main] dist_world_size=1\n",
            "[2022-03-21 23:57:05,294] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2022-03-21 23:57:07,865] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n",
            "03/21/2022 23:57:07 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "03/21/2022 23:57:07 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-06,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=ds_config.json,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=IntervalStrategy.STEPS,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.1,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output_dir/runs/Mar21_23-57-07_f03b2050fdfc,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=1000,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=output_dir,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_dir,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/wmt16/wmt16.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp2c76b1g5\n",
            "Downloading builder script: 2.81kB [00:00, 2.17MB/s]       \n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/wmt16/wmt16.py in cache at /root/.cache/huggingface/datasets/downloads/c170858f91d342822088f400f4b84ffdbe45dad7c3d7b13daf6303e87ef91a58.e23e7d1259a8c6274a82a42a8936dd1b87225302c6dc9b7261beb3bc2daac640.py\n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/c170858f91d342822088f400f4b84ffdbe45dad7c3d7b13daf6303e87ef91a58.e23e7d1259a8c6274a82a42a8936dd1b87225302c6dc9b7261beb3bc2daac640.py\n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/wmt16/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpap67ap2x\n",
            "Downloading metadata: 18.6kB [00:00, 13.4MB/s]       \n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/wmt16/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/a9a3cc86d3fbc4a3f4ac15d625dee529818a896adb8c593b7aac6fa2ec897acc.18ced3162cee0ba4303cb83507af347217937e4b03b289cd345c05957a49ffe6\n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a9a3cc86d3fbc4a3f4ac15d625dee529818a896adb8c593b7aac6fa2ec897acc.18ced3162cee0ba4303cb83507af347217937e4b03b289cd345c05957a49ffe6\n",
            "03/21/2022 23:57:09 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/wmt16/wmt_utils.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp8t0scbif\n",
            "Downloading extra modules: 41.5kB [00:00, 21.3MB/s]       \n",
            "03/21/2022 23:57:10 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/datasets/wmt16/wmt_utils.py in cache at /root/.cache/huggingface/datasets/downloads/c04367e989a7bef9b59de0b8fa736169146f259ab5e540d03d311b2ac0115a7a.2607c8683115e7ca0c7e4e18346a40bf238cdba7bcf5f6e2bd7d90193c3d50c2.py\n",
            "03/21/2022 23:57:10 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/c04367e989a7bef9b59de0b8fa736169146f259ab5e540d03d311b2ac0115a7a.2607c8683115e7ca0c7e4e18346a40bf238cdba7bcf5f6e2bd7d90193c3d50c2.py\n",
            "03/21/2022 23:57:10 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0\n",
            "03/21/2022 23:57:10 - INFO - datasets.builder - Generating dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0)\n",
            "Downloading and preparing dataset wmt16/ro-en (download: 274.05 MiB, generated: 180.62 MiB, post-processed: Unknown size, total: 454.67 MiB) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0...\n",
            "03/21/2022 23:57:10 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "Downloading data files:   0% 0/4 [00:00<?, ?it/s]03/21/2022 23:57:12 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wmt/wmt16/resolve/main-zip/translation-task/training-parallel-ep-v8.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpdzplfdge\n",
            "\n",
            "Downloading data:   0% 0.00/225M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 208k/225M [00:00<01:48, 2.07MB/s]\u001b[A\n",
            "Downloading data:   0% 1.09M/225M [00:00<00:42, 5.32MB/s]\u001b[A\n",
            "Downloading data:   1% 3.18M/225M [00:00<00:18, 11.9MB/s]\u001b[A\n",
            "Downloading data:   2% 5.46M/225M [00:00<00:13, 16.0MB/s]\u001b[A\n",
            "Downloading data:   3% 7.70M/225M [00:00<00:11, 18.3MB/s]\u001b[A\n",
            "Downloading data:   5% 10.2M/225M [00:00<00:10, 19.8MB/s]\u001b[A\n",
            "Downloading data:   6% 13.0M/225M [00:00<00:09, 22.4MB/s]\u001b[A\n",
            "Downloading data:   7% 16.2M/225M [00:00<00:08, 25.5MB/s]\u001b[A\n",
            "Downloading data:   9% 19.4M/225M [00:00<00:07, 27.6MB/s]\u001b[A\n",
            "Downloading data:  10% 23.2M/225M [00:01<00:06, 30.8MB/s]\u001b[A\n",
            "Downloading data:  12% 27.5M/225M [00:01<00:05, 34.3MB/s]\u001b[A\n",
            "Downloading data:  14% 31.8M/225M [00:01<00:05, 37.0MB/s]\u001b[A\n",
            "Downloading data:  16% 35.5M/225M [00:01<00:05, 36.6MB/s]\u001b[A\n",
            "Downloading data:  18% 40.0M/225M [00:01<00:04, 39.1MB/s]\u001b[A\n",
            "Downloading data:  20% 44.6M/225M [00:01<00:04, 41.1MB/s]\u001b[A\n",
            "Downloading data:  22% 48.7M/225M [00:01<00:04, 40.4MB/s]\u001b[A\n",
            "Downloading data:  24% 53.2M/225M [00:01<00:04, 41.8MB/s]\u001b[A\n",
            "Downloading data:  26% 57.6M/225M [00:01<00:03, 42.5MB/s]\u001b[A\n",
            "Downloading data:  27% 61.9M/225M [00:01<00:03, 42.1MB/s]\u001b[A\n",
            "Downloading data:  29% 66.1M/225M [00:02<00:03, 41.3MB/s]\u001b[A\n",
            "Downloading data:  31% 70.5M/225M [00:02<00:03, 42.1MB/s]\u001b[A\n",
            "Downloading data:  33% 74.7M/225M [00:02<00:03, 42.0MB/s]\u001b[A\n",
            "Downloading data:  35% 79.5M/225M [00:02<00:03, 43.6MB/s]\u001b[A\n",
            "Downloading data:  37% 84.1M/225M [00:02<00:03, 44.4MB/s]\u001b[A\n",
            "Downloading data:  39% 88.6M/225M [00:02<00:03, 43.8MB/s]\u001b[A\n",
            "Downloading data:  41% 93.2M/225M [00:02<00:02, 44.4MB/s]\u001b[A\n",
            "Downloading data:  43% 97.6M/225M [00:02<00:02, 44.2MB/s]\u001b[A\n",
            "Downloading data:  45% 102M/225M [00:02<00:02, 44.4MB/s] \u001b[A\n",
            "Downloading data:  47% 107M/225M [00:02<00:02, 44.7MB/s]\u001b[A\n",
            "Downloading data:  49% 111M/225M [00:03<00:02, 42.7MB/s]\u001b[A\n",
            "Downloading data:  51% 115M/225M [00:03<00:03, 36.5MB/s]\u001b[A\n",
            "Downloading data:  53% 120M/225M [00:03<00:02, 38.2MB/s]\u001b[A\n",
            "Downloading data:  55% 124M/225M [00:03<00:02, 38.8MB/s]\u001b[A\n",
            "Downloading data:  57% 128M/225M [00:03<00:02, 40.6MB/s]\u001b[A\n",
            "Downloading data:  59% 133M/225M [00:03<00:02, 41.4MB/s]\u001b[A\n",
            "Downloading data:  61% 137M/225M [00:03<00:02, 41.2MB/s]\u001b[A\n",
            "Downloading data:  63% 141M/225M [00:03<00:01, 42.2MB/s]\u001b[A\n",
            "Downloading data:  65% 146M/225M [00:03<00:01, 41.9MB/s]\u001b[A\n",
            "Downloading data:  66% 150M/225M [00:04<00:01, 40.0MB/s]\u001b[A\n",
            "Downloading data:  68% 154M/225M [00:04<00:01, 41.0MB/s]\u001b[A\n",
            "Downloading data:  70% 158M/225M [00:04<00:01, 40.9MB/s]\u001b[A\n",
            "Downloading data:  72% 162M/225M [00:04<00:01, 40.8MB/s]\u001b[A\n",
            "Downloading data:  74% 167M/225M [00:04<00:01, 41.9MB/s]\u001b[A\n",
            "Downloading data:  76% 171M/225M [00:04<00:01, 41.3MB/s]\u001b[A\n",
            "Downloading data:  78% 175M/225M [00:04<00:01, 41.9MB/s]\u001b[A\n",
            "Downloading data:  80% 180M/225M [00:04<00:01, 43.2MB/s]\u001b[A\n",
            "Downloading data:  82% 185M/225M [00:04<00:00, 44.5MB/s]\u001b[A\n",
            "Downloading data:  84% 189M/225M [00:04<00:00, 44.9MB/s]\u001b[A\n",
            "Downloading data:  86% 194M/225M [00:05<00:00, 43.8MB/s]\u001b[A\n",
            "Downloading data:  88% 198M/225M [00:05<00:00, 44.7MB/s]\u001b[A\n",
            "Downloading data:  90% 203M/225M [00:05<00:00, 44.2MB/s]\u001b[A\n",
            "Downloading data:  92% 208M/225M [00:05<00:00, 44.6MB/s]\u001b[A\n",
            "Downloading data:  94% 212M/225M [00:05<00:00, 44.7MB/s]\u001b[A\n",
            "Downloading data:  96% 217M/225M [00:05<00:00, 44.6MB/s]\u001b[A\n",
            "Downloading data: 100% 225M/225M [00:05<00:00, 39.1MB/s]\n",
            "03/21/2022 23:57:18 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wmt/wmt16/resolve/main-zip/translation-task/training-parallel-ep-v8.zip in cache at /root/.cache/huggingface/datasets/downloads/643db6243683334ce4b1c245cfc4b0b4c0ea2d8e1ece9cbfb1db13ecf6e46a7b\n",
            "03/21/2022 23:57:18 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/643db6243683334ce4b1c245cfc4b0b4c0ea2d8e1ece9cbfb1db13ecf6e46a7b\n",
            "Downloading data files:  25% 1/4 [00:08<00:25,  8.65s/it]03/21/2022 23:57:22 - INFO - datasets.utils.file_utils - https://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-ro.tmx.gz not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp30sya4q3\n",
            "\n",
            "Downloading data:   0% 0.00/23.5M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   0% 16.4k/23.5M [00:00<06:46, 57.6kB/s]\u001b[A\n",
            "Downloading data:   0% 61.4k/23.5M [00:00<03:20, 116kB/s] \u001b[A\n",
            "Downloading data:   0% 94.2k/23.5M [00:00<03:22, 116kB/s]\u001b[A\n",
            "Downloading data:   1% 193k/23.5M [00:01<01:53, 206kB/s] \u001b[A\n",
            "Downloading data:   2% 406k/23.5M [00:01<00:57, 400kB/s]\u001b[A\n",
            "Downloading data:   3% 815k/23.5M [00:01<00:30, 750kB/s]\u001b[A\n",
            "Downloading data:   7% 1.67M/23.5M [00:02<00:14, 1.48MB/s]\u001b[A\n",
            "Downloading data:  14% 3.35M/23.5M [00:02<00:06, 2.88MB/s]\u001b[A\n",
            "Downloading data:  24% 5.60M/23.5M [00:02<00:04, 4.43MB/s]\u001b[A\n",
            "Downloading data:  34% 8.01M/23.5M [00:02<00:02, 5.66MB/s]\u001b[A\n",
            "Downloading data:  44% 10.3M/23.5M [00:03<00:02, 6.42MB/s]\u001b[A\n",
            "Downloading data:  54% 12.7M/23.5M [00:03<00:01, 6.97MB/s]\u001b[A\n",
            "Downloading data:  64% 15.0M/23.5M [00:03<00:01, 7.29MB/s]\u001b[A\n",
            "Downloading data:  74% 17.4M/23.5M [00:04<00:00, 7.68MB/s]\u001b[A\n",
            "Downloading data:  84% 19.6M/23.5M [00:04<00:00, 7.67MB/s]\u001b[A\n",
            "Downloading data: 100% 23.5M/23.5M [00:04<00:00, 5.08MB/s]\n",
            "03/21/2022 23:57:29 - INFO - datasets.utils.file_utils - storing https://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-ro.tmx.gz in cache at /root/.cache/huggingface/datasets/downloads/836c8a8ca41f6cde5f353d1e9fe29eba1bfa7ce4d5840b7f3176131b65a64b7b\n",
            "03/21/2022 23:57:29 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/836c8a8ca41f6cde5f353d1e9fe29eba1bfa7ce4d5840b7f3176131b65a64b7b\n",
            "Downloading data files:  50% 2/4 [00:19<00:19,  9.67s/it]03/21/2022 23:57:30 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/wmt/wmt19/resolve/main-zip/translation-task/dev.zip not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmparw1i5tp\n",
            "\n",
            "Downloading data:   0% 0.00/38.7M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   7% 2.64M/38.7M [00:00<00:01, 26.4MB/s]\u001b[A\n",
            "Downloading data:  17% 6.69M/38.7M [00:00<00:00, 34.7MB/s]\u001b[A\n",
            "Downloading data:  28% 10.9M/38.7M [00:00<00:00, 38.2MB/s]\u001b[A\n",
            "Downloading data:  40% 15.6M/38.7M [00:00<00:00, 41.4MB/s]\u001b[A\n",
            "Downloading data:  51% 19.7M/38.7M [00:00<00:00, 41.3MB/s]\u001b[A\n",
            "Downloading data:  62% 24.0M/38.7M [00:00<00:00, 41.9MB/s]\u001b[A\n",
            "Downloading data:  74% 28.6M/38.7M [00:00<00:00, 43.2MB/s]\u001b[A\n",
            "Downloading data:  86% 33.3M/38.7M [00:00<00:00, 44.4MB/s]\u001b[A\n",
            "Downloading data: 100% 38.7M/38.7M [00:00<00:00, 42.1MB/s]\n",
            "03/21/2022 23:57:31 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/wmt/wmt19/resolve/main-zip/translation-task/dev.zip in cache at /root/.cache/huggingface/datasets/downloads/11ba2d79cea2e93248b0441726361c9bb586ed8c2b366d3eca0e2a09b09560cf\n",
            "03/21/2022 23:57:31 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/11ba2d79cea2e93248b0441726361c9bb586ed8c2b366d3eca0e2a09b09560cf\n",
            "Downloading data files: 100% 4/4 [00:21<00:00,  5.41s/it]\n",
            "03/21/2022 23:57:31 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "03/21/2022 23:57:32 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 4/4 [00:08<00:00,  2.03s/it]\n",
            "Extracting data files: 0it [00:00, ?it/s]\n",
            "03/21/2022 23:57:41 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
            "03/21/2022 23:57:41 - INFO - datasets.builder - Generating train split\n",
            "03/21/2022 23:58:06 - INFO - datasets.builder - Generating validation split\n",
            "03/21/2022 23:58:06 - INFO - datasets.builder - Generating test split\n",
            "03/21/2022 23:58:06 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 396.86it/s]\n",
            "[INFO|file_utils.py:2241] 2022-03-21 23:58:07,488 >> https://huggingface.co/t5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6su99zo2\n",
            "Downloading: 100% 1.17k/1.17k [00:00<00:00, 691kB/s]\n",
            "[INFO|file_utils.py:2245] 2022-03-21 23:58:08,275 >> storing https://huggingface.co/t5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "[INFO|file_utils.py:2253] 2022-03-21 23:58:08,275 >> creating metadata file for /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "[INFO|configuration_utils.py:649] 2022-03-21 23:58:08,276 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "[INFO|configuration_utils.py:685] 2022-03-21 23:58:08,279 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:345] 2022-03-21 23:58:09,026 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:649] 2022-03-21 23:58:09,773 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "[INFO|configuration_utils.py:685] 2022-03-21 23:58:09,774 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2241] 2022-03-21 23:58:11,269 >> https://huggingface.co/t5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpytbebpcj\n",
            "Downloading: 100% 773k/773k [00:00<00:00, 1.08MB/s]\n",
            "[INFO|file_utils.py:2245] 2022-03-21 23:58:12,935 >> storing https://huggingface.co/t5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "[INFO|file_utils.py:2253] 2022-03-21 23:58:12,936 >> creating metadata file for /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "[INFO|file_utils.py:2241] 2022-03-21 23:58:13,690 >> https://huggingface.co/t5-small/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1_hnhws4\n",
            "Downloading: 100% 1.32M/1.32M [00:00<00:00, 1.51MB/s]\n",
            "[INFO|file_utils.py:2245] 2022-03-21 23:58:15,546 >> storing https://huggingface.co/t5-small/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "[INFO|file_utils.py:2253] 2022-03-21 23:58:15,546 >> creating metadata file for /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-21 23:58:17,795 >> loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-21 23:58:17,795 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-21 23:58:17,796 >> loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-21 23:58:17,796 >> loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1786] 2022-03-21 23:58:17,796 >> loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:649] 2022-03-21 23:58:18,542 >> loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "[INFO|configuration_utils.py:685] 2022-03-21 23:58:18,543 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"t5-small\",\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.18.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2241] 2022-03-21 23:58:19,391 >> https://huggingface.co/t5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzkyutgm5\n",
            "Downloading: 100% 231M/231M [00:05<00:00, 41.3MB/s]\n",
            "[INFO|file_utils.py:2245] 2022-03-21 23:58:25,332 >> storing https://huggingface.co/t5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "[INFO|file_utils.py:2253] 2022-03-21 23:58:25,332 >> creating metadata file for /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "[INFO|modeling_utils.py:1432] 2022-03-21 23:58:25,332 >> loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "[INFO|modeling_utils.py:1703] 2022-03-21 23:58:26,577 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:1712] 2022-03-21 23:58:26,577 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "Running tokenizer on train dataset:   0% 0/2 [00:00<?, ?ba/s]03/21/2022 23:58:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0/cache-13e1dc9586f1e460.arrow\n",
            "Running tokenizer on train dataset: 100% 2/2 [00:00<00:00,  7.67ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]03/21/2022 23:58:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0/cache-ad71bba323ea7a4c.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  8.63ba/s]\n",
            "03/21/2022 23:58:28 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/sacrebleu/sacrebleu.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqiialgnd\n",
            "Downloading builder script: 5.65kB [00:00, 4.21MB/s]       \n",
            "03/21/2022 23:58:28 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.0.0/metrics/sacrebleu/sacrebleu.py in cache at /root/.cache/huggingface/datasets/downloads/905d98985b8c32c52f58f7c2e9a7a0039393125931b856f699779c94e94141b0.316789b788ba2e0a8dfae1ad2c73407add3ff5c1c3c1885370004da34596813a.py\n",
            "03/21/2022 23:58:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/905d98985b8c32c52f58f7c2e9a7a0039393125931b856f699779c94e94141b0.316789b788ba2e0a8dfae1ad2c73407add3ff5c1c3c1885370004da34596813a.py\n",
            "[INFO|trainer.py:457] 2022-03-21 23:58:28,811 >> Using amp half precision backend\n",
            "[2022-03-21 23:58:28,817] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.1+0eb2c763, git-hash=0eb2c763, git-branch=master\n",
            "[2022-03-21 23:58:31,409] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Installed CUDA version 11.1 does not match the version torch was compiled with 11.3 but since the APIs are compatible, accepting this combination\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py37_cu113/cpu_adam...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu113/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_37,code=compute_37 -gencode=arch=compute_37,code=sm_37 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_37,code=sm_37 -gencode=arch=compute_37,code=compute_37 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
            "nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
            "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -g -Wno-reorder -L/usr/local/cuda/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
            "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -lcurand -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 40.4235315322876 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2022-03-21 23:59:13,621] [INFO] [engine.py:1066:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2022-03-21 23:59:13,626] [INFO] [engine.py:1073:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2022-03-21 23:59:13,626] [INFO] [utils.py:49:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2022-03-21 23:59:13,626] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
            "[2022-03-21 23:59:13,626] [INFO] [stage_1_and_2.py:125:__init__] Reduce bucket size 200000000.0\n",
            "[2022-03-21 23:59:13,626] [INFO] [stage_1_and_2.py:126:__init__] Allgather bucket size 200000000.0\n",
            "[2022-03-21 23:59:13,626] [INFO] [stage_1_and_2.py:127:__init__] CPU Offload: True\n",
            "[2022-03-21 23:59:13,626] [INFO] [stage_1_and_2.py:128:__init__] Round robin gradient partitioning: False\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/py37_cu113/utils...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py37_cu113/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 18.837929725646973 seconds\n",
            "Rank: 0 partition count [1] and sizes[(60492288, False)] \n",
            "[2022-03-21 23:59:33,101] [INFO] [utils.py:824:see_memory_usage] Before initializing optimizer states\n",
            "[2022-03-21 23:59:33,102] [INFO] [utils.py:829:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.24 GB         Max_CA 0 GB \n",
            "[2022-03-21 23:59:33,102] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 3.4 GB, percent = 26.8%\n",
            "[2022-03-21 23:59:33,823] [INFO] [utils.py:824:see_memory_usage] After initializing optimizer states\n",
            "[2022-03-21 23:59:33,824] [INFO] [utils.py:829:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.24 GB         Max_CA 0 GB \n",
            "[2022-03-21 23:59:33,824] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 4.1 GB, percent = 32.3%\n",
            "[2022-03-21 23:59:33,824] [INFO] [stage_1_and_2.py:500:__init__] optimizer state initialized\n",
            "[2022-03-21 23:59:33,897] [INFO] [utils.py:824:see_memory_usage] After initializing ZeRO optimizer\n",
            "[2022-03-21 23:59:33,898] [INFO] [utils.py:829:see_memory_usage] MA 0.14 GB         Max_MA 0.14 GB         CA 0.24 GB         Max_CA 0 GB \n",
            "[2022-03-21 23:59:33,898] [INFO] [utils.py:834:see_memory_usage] CPU Virtual Memory:  used = 4.1 GB, percent = 32.3%\n",
            "[2022-03-21 23:59:33,898] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2022-03-21 23:59:33,898] [INFO] [engine.py:777:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2022-03-21 23:59:33,898] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fb7cef0b910>\n",
            "[2022-03-21 23:59:33,899] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
            "[2022-03-21 23:59:33,899] [INFO] [config.py:1060:print] DeepSpeedEngine configuration:\n",
            "[2022-03-21 23:59:33,900] [INFO] [config.py:1064:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2022-03-21 23:59:33,900] [INFO] [config.py:1064:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2022-03-21 23:59:33,900] [INFO] [config.py:1064:print]   amp_enabled .................. False\n",
            "[2022-03-21 23:59:33,900] [INFO] [config.py:1064:print]   amp_params ................... False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   autotuning_config ............ {\n",
            "    \"enabled\": false, \n",
            "    \"start_step\": null, \n",
            "    \"end_step\": null, \n",
            "    \"metric_path\": null, \n",
            "    \"arg_mappings\": null, \n",
            "    \"metric\": \"throughput\", \n",
            "    \"model_info\": null, \n",
            "    \"results_dir\": null, \n",
            "    \"exps_dir\": null, \n",
            "    \"overwrite\": true, \n",
            "    \"fast\": true, \n",
            "    \"start_profile_step\": 3, \n",
            "    \"end_profile_step\": 5, \n",
            "    \"tuner_type\": \"gridsearch\", \n",
            "    \"tuner_early_stopping\": 5, \n",
            "    \"tuner_num_trials\": 50, \n",
            "    \"model_info_path\": null, \n",
            "    \"mp_size\": 1, \n",
            "    \"max_train_batch_size\": null, \n",
            "    \"min_train_batch_size\": 1, \n",
            "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
            "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
            "    \"num_tuning_micro_batch_sizes\": 3\n",
            "}\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   bfloat16_enabled ............. False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   checkpoint_tag_validation_enabled  True\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   checkpoint_tag_validation_fail  False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   communication_data_type ...... None\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   curriculum_enabled ........... False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   curriculum_params ............ False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   dataloader_drop_last ......... False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   disable_allgather ............ False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   dump_state ................... False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   eigenvalue_enabled ........... False\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2022-03-21 23:59:33,901] [INFO] [config.py:1064:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   eigenvalue_layer_num ......... 0\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   eigenvalue_max_iter .......... 100\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   eigenvalue_stability ......... 1e-06\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   eigenvalue_tol ............... 0.01\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   eigenvalue_verbose ........... False\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   elasticity_enabled ........... False\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   fp16_enabled ................. True\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   fp16_master_weights_and_gradients  False\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   fp16_mixed_quantize .......... False\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   global_rank .................. 0\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   gradient_accumulation_steps .. 1\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   gradient_clipping ............ 1.0\n",
            "[2022-03-21 23:59:33,902] [INFO] [config.py:1064:print]   gradient_predivide_factor .... 1.0\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   initial_dynamic_scale ........ 65536\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   loss_scale ................... 0\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   memory_breakdown ............. False\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   optimizer_legacy_fusion ...... False\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   optimizer_name ............... adamw\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.0}\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   pld_enabled .................. False\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   pld_params ................... False\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   prescale_gradients ........... False\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   quantize_change_rate ......... 0.001\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   quantize_groups .............. 1\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   quantize_offset .............. 1000\n",
            "[2022-03-21 23:59:33,903] [INFO] [config.py:1064:print]   quantize_period .............. 1000\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   quantize_rounding ............ 0\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   quantize_start_bits .......... 16\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   quantize_target_bits ......... 8\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   quantize_training_enabled .... False\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   quantize_type ................ 0\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   quantize_verbose ............. False\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   scheduler_name ............... WarmupLR\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 3e-05, 'warmup_num_steps': 500}\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   sparse_attention ............. None\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   sparse_gradients_enabled ..... False\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   steps_per_print .............. 2000\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   tensorboard_enabled .......... False\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   tensorboard_output_path ...... \n",
            "[2022-03-21 23:59:33,904] [INFO] [config.py:1064:print]   train_batch_size ............. 16\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   train_micro_batch_size_per_gpu  16\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   use_quantizer_kernel ......... False\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   wall_clock_breakdown ......... False\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   world_size ................... 1\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   zero_allow_untested_optimizer  False\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   zero_config .................. {\n",
            "    \"stage\": 2, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 2.000000e+08, \n",
            "    \"allgather_partitions\": true, \n",
            "    \"allgather_bucket_size\": 2.000000e+08, \n",
            "    \"overlap_comm\": true, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": false, \n",
            "    \"offload_param\": null, \n",
            "    \"offload_optimizer\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 4, \n",
            "        \"pin_memory\": true, \n",
            "        \"pipeline_read\": false, \n",
            "        \"pipeline_write\": false, \n",
            "        \"fast_init\": false, \n",
            "        \"pipeline\": false\n",
            "    }, \n",
            "    \"sub_group_size\": 1.000000e+09, \n",
            "    \"prefetch_bucket_size\": 5.000000e+07, \n",
            "    \"param_persistence_threshold\": 1.000000e+05, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_16bit_weights_on_model_save\": false, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"round_robin_gradients\": false, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   zero_enabled ................. True\n",
            "[2022-03-21 23:59:33,905] [INFO] [config.py:1064:print]   zero_optimization_stage ...... 2\n",
            "[2022-03-21 23:59:33,906] [INFO] [config.py:1072:print]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 3e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-06, \n",
            "            \"weight_decay\": 0.0\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 3e-05, \n",
            "            \"warmup_num_steps\": 500\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"train_batch_size\": 16, \n",
            "    \"train_micro_batch_size_per_gpu\": 16, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "Using /root/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0007846355438232422 seconds\n",
            "[INFO|trainer.py:1288] 2022-03-21 23:59:33,907 >> ***** Running training *****\n",
            "[INFO|trainer.py:1289] 2022-03-21 23:59:33,907 >>   Num examples = 2000\n",
            "[INFO|trainer.py:1290] 2022-03-21 23:59:33,907 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1291] 2022-03-21 23:59:33,907 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1292] 2022-03-21 23:59:33,908 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1293] 2022-03-21 23:59:33,908 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1294] 2022-03-21 23:59:33,908 >>   Total optimization steps = 125\n",
            "  0% 0/125 [00:00<?, ?it/s][2022-03-21 23:59:42,809] [INFO] [stage_1_and_2.py:1660:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 65536\n",
            "  1% 1/125 [00:03<06:43,  3.25s/it][WARNING|trainer_pt_utils.py:806] 2022-03-21 23:59:42,811 >> tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
            "{'loss': 3.3297, 'learning_rate': 0, 'epoch': 0.01}\n",
            "  1% 1/125 [00:03<06:43,  3.25s/it][2022-03-21 23:59:43,490] [INFO] [stage_1_and_2.py:1660:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768.0\n",
            " 67% 84/125 [01:17<00:36,  1.11it/s][2022-03-22 00:00:58,115] [INFO] [stage_1_and_2.py:1660:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "100% 125/125 [01:55<00:00,  1.10it/s][INFO|trainer.py:1526] 2022-03-22 00:01:34,693 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 120.785, 'train_samples_per_second': 16.558, 'train_steps_per_second': 1.035, 'train_loss': 2.745602590560913, 'epoch': 1.0}\n",
            "100% 125/125 [01:55<00:00,  1.09it/s]\n",
            "[INFO|trainer.py:2162] 2022-03-22 00:01:34,695 >> Saving model checkpoint to output_dir\n",
            "[INFO|configuration_utils.py:440] 2022-03-22 00:01:34,697 >> Configuration saved in output_dir/config.json\n",
            "[INFO|modeling_utils.py:1085] 2022-03-22 00:01:35,142 >> Model weights saved in output_dir/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2094] 2022-03-22 00:01:35,143 >> tokenizer config file saved in output_dir/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2100] 2022-03-22 00:01:35,144 >> Special tokens file saved in output_dir/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:162] 2022-03-22 00:01:35,232 >> Copy vocab file to output_dir/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =     2.7456\n",
            "  train_runtime            = 0:02:00.78\n",
            "  train_samples            =       2000\n",
            "  train_samples_per_second =     16.558\n",
            "  train_steps_per_second   =      1.035\n",
            "03/22/2022 00:01:35 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2412] 2022-03-22 00:01:35,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2414] 2022-03-22 00:01:35,245 >>   Num examples = 500\n",
            "[INFO|trainer.py:2417] 2022-03-22 00:01:35,245 >>   Batch size = 16\n",
            "100% 32/32 [02:04<00:00,  4.07s/it]03/22/2022 00:03:43 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/sacrebleu/default/default_experiment-1-0.arrow\n",
            "100% 32/32 [02:04<00:00,  3.88s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_bleu               =    23.8616\n",
            "  eval_gen_len            =     39.454\n",
            "  eval_loss               =     3.3842\n",
            "  eval_runtime            = 0:02:07.93\n",
            "  eval_samples            =        500\n",
            "  eval_samples_per_second =      3.908\n",
            "  eval_steps_per_second   =       0.25\n",
            "[2022-03-22 00:03:46,721] [INFO] [launch.py:210:main] Process 488 exits successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSlYvQWLwblN"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}