{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of DeepSpeed on colab CLI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsT5mHM6VTpt"
      },
      "source": [
        "\n",
        "[**Open in Colab**](https://colab.research.google.com/github/stas00/porting/blob/master/transformers/deepspeed/DeepSpeed_on_colab_CLI.ipynb)\n",
        "\n",
        "Last modified: Thu 10 Jun 2021 04:06:56 PM\n",
        "\n",
        "# transformers + deepspeed CLI\n",
        "\n",
        "This notebook demonstrates how to setup `transformers` + `deepspeed` on colab to be run as an external process.\n",
        "\n",
        "You can of course use it under any notebook environment.\n",
        "\n",
        "It's possible to run `transformers` + `deepspeed` inside the notebook as well: \n",
        "\n",
        "**XXX**: make another notebook with a demo that isn't CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6S7Z35-TkSR"
      },
      "source": [
        "\n",
        "\n",
        "## Setting up the correct environment\n",
        "\n",
        "In order to run `transformers` with `deepspeed`, you need:\n",
        "1. enough general RAM. Different users seem to get a instance with different size of allocated general RAM. Try `!free -h` and if your process gets killed, you probably run out of memory. If you can't get enough memory you can turn `cpu_offload` off in `ds_config.json` below.\n",
        "2. matching cuda versions. Your pytorch needs to be built with the exact cuda version as you system-wide installed cuda. This is normally not needed to run `pytorch` alone, but is needed for building CUDA extensions, like DeepSpeed. You will find full documentation [here](https://huggingface.co/transformers/main_classes/trainer.html#installation-notes).\n",
        "\n",
        "Since we can't control which cuda version colab has it can be tricky to find the right matching pytorch version. So this notebook will save you time by already showing you all the required versions you need to install.\n",
        "\n",
        "Surely, this notebook will get outdated in time. So make sure you check for the latest version of it at https://github.com/stas00/porting/blob/master/transformers/deepspeed/ and please let me know if it needs to be updated if deepspeed stops building.\n",
        "\n",
        "As I mentioned earlier if Deepspeed builds but the training gets killed you got a colab instance with too little RAM. There is no need to contact me then as there is nothing I can do about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr8bCfITLOQe",
        "outputId": "88c23618-c166-4d60-873d-0a8351ec42ba"
      },
      "source": [
        "# Free colab seems to give different amount of general RAM to different users or even the same users at different times.\n",
        "\n",
        "!free -h"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        561M        6.4G        1.1M        5.8G         11G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ1iecs6SWgk",
        "outputId": "0668caf8-da6c-4fb1-8d96-2479c25ac50c"
      },
      "source": [
        "# check which nvidia drivers and cuda version is running\n",
        "\n",
        "!nvidia-smi"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Jun 10 23:06:30 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9mmhJzcgHy1",
        "outputId": "9b86aa0d-abec-44d5-f32f-41e143116bd3"
      },
      "source": [
        "# need to match the system-wide installed cuda-11 for deepspeed to compile\n",
        "# so install the matching pytorch\n",
        "\n",
        "!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.8.1+cu111\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torch-1.8.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (1982.2MB)\n",
            "\u001b[K     |█████████████▌                  | 834.1MB 1.4MB/s eta 0:13:19tcmalloc: large alloc 1147494400 bytes == 0x55b7f238a000 @  0x7f6d29fa6615 0x55b7b977ccdc 0x55b7b985c52a 0x55b7b977fafd 0x55b7b9870fed 0x55b7b97f3988 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f37f0 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b98f43e1 0x55b7b98546a9 0x55b7b97bfcc4 0x55b7b9780559 0x55b7b97f44f8 0x55b7b978130a 0x55b7b97ef3b5 0x55b7b97ee7ad 0x55b7b97813ea 0x55b7b97ef3b5 0x55b7b978130a 0x55b7b97ef3b5\n",
            "\u001b[K     |█████████████████               | 1055.7MB 1.3MB/s eta 0:12:13tcmalloc: large alloc 1434370048 bytes == 0x55b8369e0000 @  0x7f6d29fa6615 0x55b7b977ccdc 0x55b7b985c52a 0x55b7b977fafd 0x55b7b9870fed 0x55b7b97f3988 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f37f0 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b98f43e1 0x55b7b98546a9 0x55b7b97bfcc4 0x55b7b9780559 0x55b7b97f44f8 0x55b7b978130a 0x55b7b97ef3b5 0x55b7b97ee7ad 0x55b7b97813ea 0x55b7b97ef3b5 0x55b7b978130a 0x55b7b97ef3b5\n",
            "\u001b[K     |█████████████████████▋          | 1336.2MB 1.4MB/s eta 0:07:57tcmalloc: large alloc 1792966656 bytes == 0x55b7bb812000 @  0x7f6d29fa6615 0x55b7b977ccdc 0x55b7b985c52a 0x55b7b977fafd 0x55b7b9870fed 0x55b7b97f3988 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f37f0 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b98f43e1 0x55b7b98546a9 0x55b7b97bfcc4 0x55b7b9780559 0x55b7b97f44f8 0x55b7b978130a 0x55b7b97ef3b5 0x55b7b97ee7ad 0x55b7b97813ea 0x55b7b97ef3b5 0x55b7b978130a 0x55b7b97ef3b5\n",
            "\u001b[K     |███████████████████████████▎    | 1691.1MB 1.4MB/s eta 0:03:26tcmalloc: large alloc 2241208320 bytes == 0x55b8265fa000 @  0x7f6d29fa6615 0x55b7b977ccdc 0x55b7b985c52a 0x55b7b977fafd 0x55b7b9870fed 0x55b7b97f3988 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f37f0 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b97ef853 0x55b7b9871e36 0x55b7b98f43e1 0x55b7b98546a9 0x55b7b97bfcc4 0x55b7b9780559 0x55b7b97f44f8 0x55b7b978130a 0x55b7b97ef3b5 0x55b7b97ee7ad 0x55b7b97813ea 0x55b7b97ef3b5 0x55b7b978130a 0x55b7b97ef3b5\n",
            "\u001b[K     |████████████████████████████████| 1982.2MB 1.2MB/s eta 0:00:01tcmalloc: large alloc 1982177280 bytes == 0x55b8abf5c000 @  0x7f6d29fa51e7 0x55b7b97b2f37 0x55b7b977ccdc 0x55b7b985c52a 0x55b7b977fafd 0x55b7b9870fed 0x55b7b97f3988 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b978130a 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b97ee4ae\n",
            "tcmalloc: large alloc 2477727744 bytes == 0x55b996668000 @  0x7f6d29fa6615 0x55b7b977ccdc 0x55b7b985c52a 0x55b7b977fafd 0x55b7b9870fed 0x55b7b97f3988 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97ef60e 0x55b7b978130a 0x55b7b97ef60e 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b97ee4ae 0x55b7b97813ea 0x55b7b97f032a 0x55b7b97ee4ae 0x55b7b9781a81\n",
            "\u001b[K     |████████████████████████████████| 1982.2MB 3.9kB/s \n",
            "\u001b[?25hCollecting torchvision==0.9.1+cu111\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (17.6MB)\n",
            "\u001b[K     |████████████████████████████████| 17.6MB 1.2MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/aa/55/01ad9244bcd595e39cea5ce30726a7fe02fd963d07daeb136bfe7e23f0a5/torchaudio-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 7.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cu111) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cu111) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Found existing installation: torch 1.7.1+cu110\n",
            "    Uninstalling torch-1.7.1+cu110:\n",
            "      Successfully uninstalled torch-1.7.1+cu110\n",
            "  Found existing installation: torchvision 0.8.2+cu110\n",
            "    Uninstalling torchvision-0.8.2+cu110:\n",
            "      Successfully uninstalled torchvision-0.8.2+cu110\n",
            "Successfully installed torch-1.8.1+cu111 torchaudio-0.8.1 torchvision-0.9.1+cu111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aNVOVxab2Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a96eab-e218-45c1-ad96-2d750755240e"
      },
      "source": [
        "# either install the release\n",
        "#!pip install deepspeed\n",
        "# or the master \n",
        "!pip install git+https://github.com/microsoft/deepspeed\n",
        "\n",
        "# remove any previously cached deepspeed objects as they can be incompatible with this new build\n",
        "#!rm -r /root/.cache/torch_extensions/"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/microsoft/deepspeed\n",
            "  Cloning https://github.com/microsoft/deepspeed to /tmp/pip-req-build-6zgk1x0y\n",
            "  Running command git clone -q https://github.com/microsoft/deepspeed /tmp/pip-req-build-6zgk1x0y\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied (use --upgrade to upgrade): deepspeed==0.4.1+71ecf7e from git+https://github.com/microsoft/deepspeed in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (1.8.1+cu111)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (0.9.1+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (4.41.1)\n",
            "Requirement already satisfied: tensorboardX==1.8 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (1.8)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (1.10.0.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (5.4.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (20.9)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.4.1+71ecf7e) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed==0.4.1+71ecf7e) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed==0.4.1+71ecf7e) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.4.1+71ecf7e) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.4.1+71ecf7e) (3.12.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->deepspeed==0.4.1+71ecf7e) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed==0.4.1+71ecf7e) (57.0.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.4.1+71ecf7e-cp37-none-any.whl size=468578 sha256=f5294c4731cf8c43a6558b285b3b288e2879f047bed761f313a578db5af0d1d5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e8njzjyu/wheels/33/7c/6d/1ac44092dd4e4b5ddd1dec9474fed46ec3fe5588be7b6ffe9e\n",
            "Successfully built deepspeed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZQAIH70Yykn",
        "outputId": "e16a6725-2946-46bd-8b7e-8159a425764e"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "# examples change a lot so let's pick a sha that we know this notebook will work with\n",
        "# comment out/remove the next line if you want the master\n",
        "git checkout  d2753dcbec712350\n",
        "pip install -e .\n",
        "pip install -r examples/pytorch/translation/requirements.txt\n",
        "\n",
        "# if needed free up some space used by cached pip packages\n",
        "# rm -rf /root/.cache/pip\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (20.9)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.0.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (0.0.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.7.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0.dev0) (1.15.0)\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.7.0.dev0\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed transformers\n",
            "Requirement already satisfied: datasets>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 2)) (0.1.95)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 3)) (3.12.4)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 4)) (1.5.1)\n",
            "Collecting py7zr\n",
            "  Downloading https://files.pythonhosted.org/packages/db/1c/d3e3a80fa8901fc232ec11ec0f2886c7e06cf38f3f40876438ada5659211/py7zr-0.16.1-py3-none-any.whl (65kB)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.7/dist-packages (from -r examples/pytorch/translation/requirements.txt (line 6)) (1.8.1+cu111)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2021.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (0.70.11.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (4.0.1)\n",
            "Requirement already satisfied: pyarrow<4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (0.0.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (20.9)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (1.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->-r examples/pytorch/translation/requirements.txt (line 3)) (57.0.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->-r examples/pytorch/translation/requirements.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->-r examples/pytorch/translation/requirements.txt (line 4)) (2.0.0)\n",
            "Collecting pyzstd<0.15.0,>=0.14.4\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/e9/fe897f8bb96163645a5b2d3a60ff8bfa6fcdedff4691a3c6c861b0324ef4/pyzstd-0.14.4-cp37-cp37m-manylinux2014_x86_64.whl (2.2MB)\n",
            "Collecting bcj-cffi<0.6.0,>=0.5.1\n",
            "  Downloading https://files.pythonhosted.org/packages/ab/1f/408e7b01375c863e01c25b1475d628deab7c9f85aeb74cced4caa5a512ce/bcj_cffi-0.5.1-cp37-cp37m-manylinux2014_x86_64.whl\n",
            "Collecting texttable\n",
            "  Downloading https://files.pythonhosted.org/packages/06/f5/46201c428aebe0eecfa83df66bf3e6caa29659dbac5a56ddfd83cae0d4a4/texttable-1.6.3-py2.py3-none-any.whl\n",
            "Collecting multivolumefile>=0.2.3\n",
            "  Downloading https://files.pythonhosted.org/packages/22/31/ec5f46fd4c83185b806aa9c736e228cb780f13990a9cf4da0beb70025fcc/multivolumefile-0.2.3-py3-none-any.whl\n",
            "Collecting pyppmd>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5e/25/d8a1f334cb3b84b734b52158233f036c1dd61c8649b90c9a3cd39be9309e/pyppmd-0.15.0-cp37-cp37m-manylinux2014_x86_64.whl (121kB)\n",
            "Collecting brotli>=1.0.9; platform_python_implementation == \"CPython\"\n",
            "  Downloading https://files.pythonhosted.org/packages/15/ea/5bd575511b37bbd1c794606a0a621e6feff8e96b7dd007a86a5d218b2d94/Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Downloading https://files.pythonhosted.org/packages/e5/9d/99a949925b5fc9604cb65219951fd270ef30d0fd4f064d1b363eb8bb5e9b/pycryptodomex-3.10.1-cp35-abi3-manylinux2010_x86_64.whl (1.9MB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3->-r examples/pytorch/translation/requirements.txt (line 6)) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r examples/pytorch/translation/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: cffi>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from bcj-cffi<0.6.0,>=0.5.1->py7zr->-r examples/pytorch/translation/requirements.txt (line 5)) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.14.0->bcj-cffi<0.6.0,>=0.5.1->py7zr->-r examples/pytorch/translation/requirements.txt (line 5)) (2.20)\n",
            "Installing collected packages: pyzstd, bcj-cffi, texttable, multivolumefile, pyppmd, brotli, pycryptodomex, py7zr\n",
            "Successfully installed bcj-cffi-0.5.1 brotli-1.0.9 multivolumefile-0.2.3 py7zr-0.16.1 pycryptodomex-3.10.1 pyppmd-0.15.0 pyzstd-0.14.4 texttable-1.6.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "HEAD is now at d2753dcbe add relevant description to tqdm in examples (#11927)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYG2EDAQdFxt"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd transformers\n",
        "\n",
        "cat <<'EOT' > ds_config.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"allgather_partitions\": true,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": true,\n",
        "        \"reduce_scatter\": true,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"contiguous_gradients\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJKdum6zdxLE"
      },
      "source": [
        "#!ls -l transformers\n",
        "#!cat transformers/ds_config.json"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XJEYx1sVuAJ"
      },
      "source": [
        "## Running Traning + Evaluation CLI style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghNfx0nNZSfq",
        "outputId": "506b3cb3-7d7e-4c7c-ab69-801a93d7d783"
      },
      "source": [
        "!cd transformers; export BS=16; rm -r output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n",
        "--model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps \\\n",
        "--do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 \\\n",
        "--max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir  \\\n",
        "--per_device_train_batch_size $BS --per_device_eval_batch_size $BS --predict_with_generate --sortish_sampler \\\n",
        "--val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_eval_samples 500 \\\n",
        "--dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro \\\n",
        "--source_prefix \"translate English to Romanian: \" --deepspeed ds_config.json --fp16"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-10 23:00:33,454] [WARNING] [runner.py:122:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-06-10 23:00:33,483] [INFO] [runner.py:360:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 examples/pytorch/translation/run_translation.py --model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps --do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --predict_with_generate --sortish_sampler --val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_eval_samples 500 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro --source_prefix translate English to Romanian:  --deepspeed ds_config.json --fp16\n",
            "[2021-06-10 23:00:34,618] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.7.8\n",
            "[2021-06-10 23:00:34,618] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-06-10 23:00:34,618] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-06-10 23:00:34,618] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-06-10 23:00:34,618] [INFO] [launch.py:102:main] dist_world_size=1\n",
            "[2021-06-10 23:00:34,618] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2021-06-10 23:00:36,902] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n",
            "WARNING:__main__:Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-06,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=ds_config.json,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=1000,\n",
            "evaluation_strategy=IntervalStrategy.STEPS,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.1,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_on_each_node=True,\n",
            "logging_dir=runs/Jun10_23-00-36_5a08d4383207,\n",
            "logging_first_step=True,\n",
            "logging_steps=1000,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "output_dir=output_dir,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=16,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output_dir,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=500,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "WARNING:datasets.builder:Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a)\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.7.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-3e6065c753dc0cfd.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/0d9fb3e814712c785176ad8cdb9f465fbe6479000ee6546725db30ad8a8b5f8a/cache-eda13657ebc5e017.arrow\n",
            "Using amp fp16 backend\n",
            "[2021-06-10 23:00:41,697] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.4.1+71ecf7e, git-hash=71ecf7e, git-branch=master\n",
            "[2021-06-10 23:00:45,529] [INFO] [utils.py:13:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "[2021-06-10 23:00:45,596] [INFO] [engine.py:173:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "Installed CUDA version 11.0 does not match the version torch was compiled with 11.1 but since the APIs are compatible, accepting this combination\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/cpu_adam...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output custom_cuda_kernel.cuda.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_75,code=sm_75 -gencode=arch=compute_75,code=compute_75 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
            "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -L/usr/local/cuda/lib64 -lcudart -lcublas -g -Wno-reorder -march=native -fopenmp -D__AVX256__ -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
            "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 27.42343783378601 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2021-06-10 23:01:15,570] [INFO] [engine.py:693:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2021-06-10 23:01:15,571] [INFO] [engine.py:697:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2021-06-10 23:01:15,571] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
            "[2021-06-10 23:01:15,571] [INFO] [stage2.py:105:__init__] Reduce bucket size 200000000.0\n",
            "[2021-06-10 23:01:15,571] [INFO] [stage2.py:106:__init__] Allgather bucket size 200000000.0\n",
            "[2021-06-10 23:01:15,571] [INFO] [stage2.py:107:__init__] CPU Offload: True\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/utils...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 13.71970272064209 seconds\n",
            "[2021-06-10 23:01:30,262] [INFO] [stage2.py:409:__init__] optimizer state initialized\n",
            "[2021-06-10 23:01:30,262] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2021-06-10 23:01:30,262] [INFO] [engine.py:505:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-06-10 23:01:30,262] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f577c8cea50>\n",
            "[2021-06-10 23:01:30,262] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
            "[2021-06-10 23:01:30,262] [INFO] [config.py:900:print] DeepSpeedEngine configuration:\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   allreduce_always_fp32 ........ False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   amp_enabled .................. False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   amp_params ................... False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   checkpoint_tag_validation_enabled  True\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   checkpoint_tag_validation_fail  False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   disable_allgather ............ False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   dump_state ................... False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   eigenvalue_enabled ........... False\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2021-06-10 23:01:30,263] [INFO] [config.py:904:print]   eigenvalue_layer_num ......... 0\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   eigenvalue_max_iter .......... 100\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   eigenvalue_stability ......... 1e-06\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   eigenvalue_tol ............... 0.01\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   eigenvalue_verbose ........... False\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   elasticity_enabled ........... False\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   fp16_enabled ................. True\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   fp16_mixed_quantize .......... False\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   global_rank .................. 0\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   gradient_accumulation_steps .. 1\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   gradient_clipping ............ 1.0\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   initial_dynamic_scale ........ 65536\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   loss_scale ................... 0\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   memory_breakdown ............. False\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   optimizer_name ............... adamw\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.0}\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   pld_enabled .................. False\n",
            "[2021-06-10 23:01:30,264] [INFO] [config.py:904:print]   pld_params ................... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   prescale_gradients ........... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_change_rate ......... 0.001\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_groups .............. 1\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_offset .............. 1000\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_period .............. 1000\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_rounding ............ 0\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_start_bits .......... 16\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_target_bits ......... 8\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_training_enabled .... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_type ................ 0\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   quantize_verbose ............. False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   scheduler_name ............... WarmupLR\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 3e-05, 'warmup_num_steps': 500}\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   sparse_attention ............. None\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   sparse_gradients_enabled ..... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   steps_per_print .............. 2000\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   tensorboard_enabled .......... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   tensorboard_output_path ...... \n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   train_batch_size ............. 16\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   train_micro_batch_size_per_gpu  16\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   use_quantizer_kernel ......... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   wall_clock_breakdown ......... False\n",
            "[2021-06-10 23:01:30,265] [INFO] [config.py:904:print]   world_size ................... 1\n",
            "[2021-06-10 23:01:30,266] [INFO] [config.py:904:print]   zero_allow_untested_optimizer  False\n",
            "[2021-06-10 23:01:30,266] [INFO] [config.py:904:print]   zero_config .................. {\n",
            "    \"stage\": 2, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 2.000000e+08, \n",
            "    \"allgather_partitions\": true, \n",
            "    \"allgather_bucket_size\": 2.000000e+08, \n",
            "    \"overlap_comm\": true, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": true, \n",
            "    \"offload_param\": null, \n",
            "    \"offload_optimizer\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 4, \n",
            "        \"pin_memory\": true, \n",
            "        \"pipeline_read\": false, \n",
            "        \"pipeline_write\": false, \n",
            "        \"fast_init\": false, \n",
            "        \"pipeline\": false\n",
            "    }, \n",
            "    \"sub_group_size\": 1.000000e+12, \n",
            "    \"prefetch_bucket_size\": 5.000000e+07, \n",
            "    \"param_persistence_threshold\": 1.000000e+05, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_fp16_weights_on_model_save\": false, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2021-06-10 23:01:30,266] [INFO] [config.py:904:print]   zero_enabled ................. True\n",
            "[2021-06-10 23:01:30,266] [INFO] [config.py:904:print]   zero_optimization_stage ...... 2\n",
            "[2021-06-10 23:01:30,266] [INFO] [config.py:911:print]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 3e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-06, \n",
            "            \"weight_decay\": 0.0\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 3e-05, \n",
            "            \"warmup_num_steps\": 500\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"stage\": 2, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"allgather_partitions\": true, \n",
            "        \"allgather_bucket_size\": 2.000000e+08, \n",
            "        \"overlap_comm\": true, \n",
            "        \"reduce_scatter\": true, \n",
            "        \"reduce_bucket_size\": 2.000000e+08, \n",
            "        \"contiguous_gradients\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"train_batch_size\": 16, \n",
            "    \"train_micro_batch_size_per_gpu\": 16, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0005445480346679688 seconds\n",
            "***** Running training *****\n",
            "  Num examples = 2000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 125\n",
            "2021-06-10 23:01:32.716353: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "  0% 0/125 [00:00<?, ?it/s][2021-06-10 23:01:41,484] [INFO] [stage2.py:1506:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 65536\n",
            "  1% 1/125 [00:04<09:25,  4.56s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
            "{'loss': 3.3036, 'learning_rate': 0, 'epoch': 0.01}\n",
            "  1% 1/125 [00:04<09:25,  4.56s/it][2021-06-10 23:01:41,658] [INFO] [stage2.py:1506:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768.0\n",
            " 74% 93/125 [00:30<00:09,  3.53it/s][2021-06-10 23:02:07,460] [INFO] [stage2.py:1506:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "100% 125/125 [00:39<00:00,  3.57it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 45.8983, 'train_samples_per_second': 43.575, 'train_steps_per_second': 2.723, 'train_loss': 2.741039337158203, 'epoch': 1.0}\n",
            "100% 125/125 [00:39<00:00,  3.19it/s]\n",
            "Saving model checkpoint to output_dir\n",
            "Configuration saved in output_dir/config.json\n",
            "Model weights saved in output_dir/pytorch_model.bin\n",
            "tokenizer config file saved in output_dir/tokenizer_config.json\n",
            "Special tokens file saved in output_dir/special_tokens_map.json\n",
            "Copy vocab file to output_dir/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  train_loss               =      2.741\n",
            "  train_runtime            = 0:00:45.89\n",
            "  train_samples            =       2000\n",
            "  train_samples_per_second =     43.575\n",
            "  train_steps_per_second   =      2.723\n",
            "INFO:__main__:*** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "100% 32/32 [00:33<00:00,  1.06s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_bleu               =     23.861\n",
            "  eval_gen_len            =     39.326\n",
            "  eval_loss               =     3.3846\n",
            "  eval_runtime            = 0:00:34.81\n",
            "  eval_samples            =        500\n",
            "  eval_samples_per_second =      14.36\n",
            "  eval_steps_per_second   =      0.919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSlYvQWLwblN"
      },
      "source": [
        ""
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}