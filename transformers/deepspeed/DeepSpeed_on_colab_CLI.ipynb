{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of DeepSpeed on colab CLI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsT5mHM6VTpt"
      },
      "source": [
        "# transformers + deepspeed CLI\n",
        "\n",
        "This notebook demonstrates how to setup `transformers` + `deepspeed` on colab to be run as an external process.\n",
        "\n",
        "You can of course use it under any notebook environment.\n",
        "\n",
        "It's possible to run `transformers` + `deepspeed` inside the notebook as well: \n",
        "\n",
        "**XXX**: make another notebook with a demo that isn't CLI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6S7Z35-TkSR"
      },
      "source": [
        "\n",
        "\n",
        "## Setting up the correct environment\n",
        "\n",
        "In order to run `transformers` with `deepspeed`, you need:\n",
        "1. enough general RAM. Different users seem to get a instance with different size of allocated general RAM. Try `!free -h` and if your process gets killed, you probably run out of memory. If you can't get enough memory you can turn `cpu_offload` off in `ds_config.json` below.\n",
        "2. matching cuda versions. Your pytorch needs to be built with the exact cuda version as you system-wide installed cuda. This is normally not needed to run `pytorch` alone, but is needed for building CUDA extensions, like DeepSpeed. You will find full documentation [here](https://huggingface.co/transformers/main_classes/trainer.html#installation-notes).\n",
        "\n",
        "Since we can't control which cuda version colab has it can be tricky to find the right matching pytorch version. So this notebook will save you time by already showing you all the required versions you need to install.\n",
        "\n",
        "Surely, this notebook will get outdated in time. So make sure you check for the latest version of it at https://github.com/stas00/porting/blob/master/transformers/deepspeed/ and please let me know if it needs to be updated if deepspeed stops building.\n",
        "\n",
        "As I mentioned earlier if Deepspeed builds but the training gets killed you got a colab instance with too little RAM. There is no need to contact me then as there is nothing I can do about it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ1iecs6SWgk",
        "outputId": "543f188a-4ba2-4dbf-ba51-dc261be65c13"
      },
      "source": [
        "# Free colab seems to give different \n",
        "\n",
        "!free -h"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            25G        676M         18G        1.0M        6.6G         24G\n",
            "Swap:            0B          0B          0B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9mmhJzcgHy1",
        "outputId": "810859a0-74a2-4654-d06e-b54198a6eaae"
      },
      "source": [
        "# need to match the system-wide installed cuda-11 for deepspeed to compile\n",
        "# so install the matching pytorch\n",
        "\n",
        "# pt-1.8 doesn't have cu110 build at the moment. \n",
        "# colab will eventually upgrade to cuda-11.1 and then we can use 1.8.0+cu111\n",
        "# \n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 56.3MB/s eta 0:00:06tcmalloc: large alloc 1147494400 bytes == 0x56174883e000 @  0x7fadae5a6615 0x56170fc4206c 0x56170fd21eba 0x56170fc44e8d 0x56170fd3699d 0x56170fcb8fe9 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb8e50 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb586a 0x56170fd377c6 0x56170fcb4ee2 0x56170fd377c6 0x56170fcb4ee2 0x56170fd377c6 0x56170fcb4ee2 0x56170fd377c6 0x56170fdb9431 0x56170fd1a049 0x56170fc84c84 0x56170fc458e9 0x56170fcb9ade 0x56170fc4669a 0x56170fcb4a45 0x56170fcb3e0d 0x56170fc4677a 0x56170fcb4a45 0x56170fc4669a 0x56170fcb4a45\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.2MB/s eta 0:01:26tcmalloc: large alloc 1434370048 bytes == 0x56178ce94000 @  0x7fadae5a6615 0x56170fc4206c 0x56170fd21eba 0x56170fc44e8d 0x56170fd3699d 0x56170fcb8fe9 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb8e50 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb586a 0x56170fd377c6 0x56170fcb4ee2 0x56170fd377c6 0x56170fcb4ee2 0x56170fd377c6 0x56170fcb4ee2 0x56170fd377c6 0x56170fdb9431 0x56170fd1a049 0x56170fc84c84 0x56170fc458e9 0x56170fcb9ade 0x56170fc4669a 0x56170fcb4a45 0x56170fcb3e0d 0x56170fc4677a 0x56170fcb4a45 0x56170fc4669a 0x56170fcb4a45\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 73.2MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x5617e2680000 @  0x7fadae5a6615 0x56170fc4206c 0x56170fd21eba 0x56170fc44e8d 0x56170fd3699d 0x56170fcb8fe9 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb4c9e 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb4c9e 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb4c9e 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb4c9e 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb4c9e 0x56170fc4669a 0x56170fcb4c9e 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb586a 0x56170fcb3b0e 0x56170fc4677a 0x56170fcb586a 0x56170fcb3b0e 0x56170fc46e11\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 15kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torchvision-0.8.2%2Bcu110-cp37-cp37m-linux_x86_64.whl (12.9MB)\n",
            "\u001b[K     |████████████████████████████████| 12.9MB 235kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2+cu110) (7.0.0)\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.7.1+cu110 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "Successfully installed torch-1.7.1+cu110 torchvision-0.8.2+cu110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aNVOVxab2Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415f954d-178f-465a-e08e-eac4e4be9b52"
      },
      "source": [
        "# either install the release\n",
        "#!pip install deepspeed\n",
        "# or the master \n",
        "!pip install git+https://github.com/microsoft/deepspeed"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/microsoft/deepspeed\n",
            "  Cloning https://github.com/microsoft/deepspeed to /tmp/pip-req-build-i3k2b31c\n",
            "  Running command git clone -q https://github.com/microsoft/deepspeed /tmp/pip-req-build-i3k2b31c\n",
            "  Running command git submodule update --init --recursive -q\n",
            "Requirement already satisfied (use --upgrade to upgrade): deepspeed==0.3.13+22d5a1f from git+https://github.com/microsoft/deepspeed in /usr/local/lib/python3.7/dist-packages\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (1.7.1+cu110)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (0.8.2+cu110)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (4.41.1)\n",
            "Requirement already satisfied: tensorboardX==1.8 in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (1.8)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (1.10.0.post2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (1.19.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from deepspeed==0.3.13+22d5a1f) (5.4.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed==0.3.13+22d5a1f) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed==0.3.13+22d5a1f) (7.0.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.13+22d5a1f) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed==0.3.13+22d5a1f) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed==0.3.13+22d5a1f) (54.1.2)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.13+22d5a1f-cp37-none-any.whl size=341169 sha256=aa960aa10b43e7ed9e3bec1a4c9c181b8f94e1a7b7cd8edc48e3a995e67c4598\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9vyb3bp_/wheels/33/7c/6d/1ac44092dd4e4b5ddd1dec9474fed46ec3fe5588be7b6ffe9e\n",
            "Successfully built deepspeed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZQAIH70Yykn",
        "outputId": "bf961331-5e18-49bd-ba5c-d848d7d4b41e"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/transformers\n",
        "cd transformers\n",
        "# examples change a lot so let's pick a sha that we know this notebook will work with\n",
        "# comment out/remove the next line if you want the master\n",
        "git checkout 1c06240e1b34777281\n",
        "pip install -e .\n",
        "pip install -r examples/_tests_requirements.txt\n",
        "\n",
        "# if needed free up some space used by cached pip packages\n",
        "# rm -rf /root/.cache/pip\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.0.dev0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0.dev0) (1.15.0)\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 4.4.0.dev0\n",
            "    Can't uninstall 'transformers'. No files were found to uninstall.\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed transformers\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 3)) (1.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 4)) (5.4.8)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 5)) (1.5.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 6)) (0.0.4)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 7)) (4.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: git-python==1.0.3 in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 9)) (1.0.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 10)) (1.7.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 11)) (0.79.0)\n",
            "Requirement already satisfied: elasticsearch in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 12)) (7.12.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 13)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 14)) (1.1.5)\n",
            "Requirement already satisfied: datasets>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 15)) (1.5.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 16)) (0.4.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 17)) (3.6.4)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 18)) (4.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 19)) (0.1.95)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 20)) (3.12.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.4.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (54.1.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.27.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r examples/_tests_requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r examples/_tests_requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->-r examples/_tests_requirements.txt (line 5)) (2.0.0)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (5.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.28.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.1.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.3.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (4.41.1)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (20.3.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (2.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (2.8.1)\n",
            "Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from git-python==1.0.3->-r examples/_tests_requirements.txt (line 9)) (3.1.14)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (0.18.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (2.1.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (1.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (0.10.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (4.1.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (1.5.1)\n",
            "Requirement already satisfied: watchdog; platform_system != \"Darwin\" in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (2.0.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (0.6.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (4.2.1)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (0.8.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (5.1.1)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (7.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (20.9)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r examples/_tests_requirements.txt (line 12)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r examples/_tests_requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r examples/_tests_requirements.txt (line 14)) (2018.9)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (0.0.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (3.7.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (0.8.7)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (0.70.11.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r examples/_tests_requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r examples/_tests_requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r examples/_tests_requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (3.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (1.53.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->git-python==1.0.3->-r examples/_tests_requirements.txt (line 9)) (4.0.5)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit->-r examples/_tests_requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (2.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (2.11.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=5.1.2; python_version >= \"3.4\" in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.5.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (7.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (3.7.4.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r examples/_tests_requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->git-python==1.0.3->-r examples/_tests_requirements.txt (line 9)) (3.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.3.5)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.1.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (4.8.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (22.0.3)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.6.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.9.2)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.5.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.4.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.5.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "Previous HEAD position was 1aa9c13f7 Fix GPU tests with speech\n",
            "HEAD is now at 1c06240e1 Update training args ignore_skip_data -> ignore_data_skip (#10891)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYG2EDAQdFxt"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd transformers\n",
        "\n",
        "cat <<'EOT' > ds_config.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": true,\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,        \n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "       \"allgather_partitions\": true,\n",
        "       \"allgather_bucket_size\": 2e8,\n",
        "       \"reduce_scatter\": true,\n",
        "       \"reduce_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"cpu_offload\": true\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": 3e-5,\n",
        "            \"betas\": [ 0.9, 0.999 ],\n",
        "            \"eps\": 1e-8,\n",
        "            \"weight_decay\": 3e-7\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": 0,\n",
        "            \"warmup_max_lr\": 3e-5,\n",
        "            \"warmup_num_steps\": 500\n",
        "        }\n",
        "    },\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJKdum6zdxLE"
      },
      "source": [
        "#!ls -l transformers\n",
        "#!cat transformers/ds_config.json"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XJEYx1sVuAJ"
      },
      "source": [
        "## Running Traning + Evaluation CLI style"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghNfx0nNZSfq",
        "outputId": "54cf949f-2724-48bd-9ed8-bbe6bf0737f5"
      },
      "source": [
        "!cd transformers; export BS=16; rm -r output_dir; \\\n",
        "PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/seq2seq/run_translation.py \\\n",
        "--model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps \\\n",
        "--do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 \\\n",
        "--max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir  \\\n",
        "--per_device_train_batch_size $BS --per_device_eval_batch_size $BS --predict_with_generate --sortish_sampler \\\n",
        "--val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_val_samples 500 \\\n",
        "--dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro \\\n",
        "--source_prefix \"translate English to Romanian: \" --deepspeed ds_config.json --fp16"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-03-25 03:13:20,889] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-03-25 03:13:20,908] [INFO] [runner.py:358:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 examples/seq2seq/run_translation.py --model_name_or_path t5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps --do_train --do_eval --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --predict_with_generate --sortish_sampler --val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_val_samples 500 --dataset_name wmt16 --dataset_config ro-en --source_lang en --target_lang ro --source_prefix translate English to Romanian:  --deepspeed ds_config.json --fp16\n",
            "[2021-03-25 03:13:22,193] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.7.8\n",
            "[2021-03-25 03:13:22,194] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-03-25 03:13:22,194] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-03-25 03:13:22,194] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-03-25 03:13:22,194] [INFO] [launch.py:102:main] dist_world_size=1\n",
            "[2021-03-25 03:13:22,194] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2021-03-25 03:13:24,812] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n",
            "03/25/2021 03:13:27 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "03/25/2021 03:13:27 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='output_dir', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=500, logging_dir='runs/Mar25_03-13-24_25e4530d4e51', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=1000, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='output_dir', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed='ds_config.json', label_smoothing_factor=0.1, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=True, predict_with_generate=True)\n",
            "03/25/2021 03:13:27 - WARNING - datasets.builder -   Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f)\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
            "Model config T5Config {\n",
            "  \"architectures\": [\n",
            "    \"T5WithLMHeadModel\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"relu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"n_positions\": 512,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 8,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"task_specific_params\": {\n",
            "    \"summarization\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"length_penalty\": 2.0,\n",
            "      \"max_length\": 200,\n",
            "      \"min_length\": 30,\n",
            "      \"no_repeat_ngram_size\": 3,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"summarize: \"\n",
            "    },\n",
            "    \"translation_en_to_de\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to German: \"\n",
            "    },\n",
            "    \"translation_en_to_fr\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to French: \"\n",
            "    },\n",
            "    \"translation_en_to_ro\": {\n",
            "      \"early_stopping\": true,\n",
            "      \"max_length\": 300,\n",
            "      \"num_beams\": 4,\n",
            "      \"prefix\": \"translate English to Romanian: \"\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.5.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32128\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/t5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/65fc04e21f45f61430aea0c4fedffac16a4d20d78b8e6601d8d996ebefefecd2.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/06779097c78e12f47ef67ecb728810c2ae757ee0a9efe9390c6419783d99382d.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
            "loading file https://huggingface.co/t5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/t5-small/resolve/main/tokenizer_config.json from cache at None\n",
            "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
            "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "03/25/2021 03:13:30 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-b9a14b3e82199fa3.arrow\n",
            "03/25/2021 03:13:30 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f/cache-e040be9bde7ed960.arrow\n",
            "Using amp fp16 backend\n",
            "Updating the `scheduler` config from ds_config.json with other command line arguments\n",
            "setting optimizer.params.lr to 3e-05\n",
            "setting optimizer.params.betas to [0.9, 0.999]\n",
            "setting optimizer.params.eps to 1e-06\n",
            "setting optimizer.params.weight_decay to 0.0\n",
            "Updating the `scheduler` config from ds_config.json with other command line arguments\n",
            "setting scheduler.params.warmup_max_lr to 3e-05\n",
            "setting scheduler.params.warmup_num_steps to 500\n",
            "Keeping the `fp16` config from ds_config.json intact, ignoring any fp16-specific cl args\n",
            "[2021-03-25 03:13:31,307] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.13+22d5a1f, git-hash=22d5a1f, git-branch=master\n",
            "[2021-03-25 03:13:31,427] [INFO] [engine.py:77:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "ninja: no work to do.\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 0.635383129119873 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2021-03-25 03:13:37,132] [INFO] [engine.py:602:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2021-03-25 03:13:37,132] [INFO] [engine.py:606:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2021-03-25 03:13:37,132] [INFO] [logging.py:60:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "ninja: no work to do.\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.45702648162841797 seconds\n",
            "[2021-03-25 03:13:37,590] [INFO] [stage2.py:130:__init__] Reduce bucket size 200000000.0\n",
            "[2021-03-25 03:13:37,590] [INFO] [stage2.py:131:__init__] Allgather bucket size 200000000.0\n",
            "[2021-03-25 03:13:37,590] [INFO] [stage2.py:132:__init__] CPU Offload: True\n",
            "[2021-03-25 03:13:38,266] [INFO] [stage2.py:399:__init__] optimizer state initialized\n",
            "[2021-03-25 03:13:38,266] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2021-03-25 03:13:38,267] [INFO] [engine.py:439:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-03-25 03:13:38,267] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f3dff94e710>\n",
            "[2021-03-25 03:13:38,267] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-25 03:13:38,267] [INFO] [config.py:737:print] DeepSpeedEngine configuration:\n",
            "[2021-03-25 03:13:38,267] [INFO] [config.py:741:print]   activation_checkpointing_config  {\n",
            "    \"contiguous_memory_optimization\": false,\n",
            "    \"cpu_checkpointing\": false,\n",
            "    \"number_checkpoints\": null,\n",
            "    \"partition_activations\": false,\n",
            "    \"profile\": false,\n",
            "    \"synchronize_checkpoint_boundary\": false\n",
            "}\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   allreduce_always_fp32 ........ False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   amp_enabled .................. False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   amp_params ................... False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   checkpoint_tag_validation_enabled  True\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   checkpoint_tag_validation_fail  False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   disable_allgather ............ False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   dump_state ................... False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   elasticity_enabled ........... False\n",
            "[2021-03-25 03:13:38,268] [INFO] [config.py:741:print]   flops_profiler_config ........ {\n",
            "    \"detailed\": true,\n",
            "    \"enabled\": false,\n",
            "    \"module_depth\": -1,\n",
            "    \"profile_step\": 1,\n",
            "    \"top_modules\": 3\n",
            "}\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   fp16_enabled ................. True\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   global_rank .................. 0\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   gradient_accumulation_steps .. 1\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   gradient_clipping ............ 1.0\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   initial_dynamic_scale ........ 65536\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   loss_scale ................... 0\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   memory_breakdown ............. False\n",
            "[2021-03-25 03:13:38,269] [INFO] [config.py:741:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   optimizer_name ............... adamw\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.0}\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   pld_enabled .................. False\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   pld_params ................... False\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   prescale_gradients ........... False\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   scheduler_name ............... WarmupLR\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 3e-05, 'warmup_num_steps': 500}\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   sparse_attention ............. None\n",
            "[2021-03-25 03:13:38,270] [INFO] [config.py:741:print]   sparse_gradients_enabled ..... False\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   steps_per_print .............. 2000\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   tensorboard_enabled .......... False\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   tensorboard_output_path ...... \n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   train_batch_size ............. 16\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   train_micro_batch_size_per_gpu  16\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   wall_clock_breakdown ......... False\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   world_size ................... 1\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   zero_allow_untested_optimizer  False\n",
            "[2021-03-25 03:13:38,271] [INFO] [config.py:741:print]   zero_config .................. {\n",
            "    \"allgather_bucket_size\": 200000000.0,\n",
            "    \"allgather_partitions\": true,\n",
            "    \"contiguous_gradients\": true,\n",
            "    \"cpu_offload\": true,\n",
            "    \"cpu_offload_params\": false,\n",
            "    \"cpu_offload_use_pin_memory\": false,\n",
            "    \"elastic_checkpoint\": true,\n",
            "    \"load_from_fp32_weights\": true,\n",
            "    \"max_live_parameters\": 1000000000,\n",
            "    \"max_reuse_distance\": 1000000000,\n",
            "    \"overlap_comm\": true,\n",
            "    \"param_persistence_threshold\": 100000,\n",
            "    \"prefetch_bucket_size\": 50000000,\n",
            "    \"reduce_bucket_size\": 200000000.0,\n",
            "    \"reduce_scatter\": true,\n",
            "    \"stage\": 2,\n",
            "    \"sub_group_size\": 1000000000000\n",
            "}\n",
            "[2021-03-25 03:13:38,272] [INFO] [config.py:741:print]   zero_enabled ................. True\n",
            "[2021-03-25 03:13:38,272] [INFO] [config.py:741:print]   zero_optimization_stage ...... 2\n",
            "[2021-03-25 03:13:38,272] [INFO] [config.py:747:print]   json = {\n",
            "    \"fp16\":{\n",
            "        \"enabled\":true,\n",
            "        \"hysteresis\":2,\n",
            "        \"initial_scale_power\":16,\n",
            "        \"loss_scale\":0,\n",
            "        \"loss_scale_window\":1000,\n",
            "        \"min_loss_scale\":1\n",
            "    },\n",
            "    \"gradient_accumulation_steps\":1,\n",
            "    \"gradient_clipping\":1.0,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"betas\":[\n",
            "                0.9,\n",
            "                0.999\n",
            "            ],\n",
            "            \"eps\":1e-06,\n",
            "            \"lr\":3e-05,\n",
            "            \"weight_decay\":0.0\n",
            "        },\n",
            "        \"type\":\"AdamW\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":3e-05,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":500\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"steps_per_print\":2000,\n",
            "    \"train_micro_batch_size_per_gpu\":16,\n",
            "    \"wall_clock_breakdown\":false,\n",
            "    \"zero_optimization\":{\n",
            "        \"allgather_bucket_size\":200000000.0,\n",
            "        \"allgather_partitions\":true,\n",
            "        \"contiguous_gradients\":true,\n",
            "        \"cpu_offload\":true,\n",
            "        \"overlap_comm\":true,\n",
            "        \"reduce_bucket_size\":200000000.0,\n",
            "        \"reduce_scatter\":true,\n",
            "        \"stage\":2\n",
            "    }\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0007305145263671875 seconds\n",
            "***** Running training *****\n",
            "  Num examples = 2000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 125\n",
            "2021-03-25 03:13:38.410951: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "  0% 0/125 [00:00<?, ?it/s][2021-03-25 03:13:40,984] [INFO] [stage2.py:1391:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 65536\n",
            "  1% 1/125 [00:00<01:18,  1.59it/s]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
            "{'loss': 3.2891, 'learning_rate': 0, 'epoch': 0.01}\n",
            "  1% 1/125 [00:00<01:18,  1.59it/s][2021-03-25 03:13:41,157] [INFO] [stage2.py:1391:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768.0\n",
            " 52% 65/125 [00:21<00:19,  3.07it/s][2021-03-25 03:14:02,087] [INFO] [stage2.py:1391:step] [deepspeed] fp16 dynamic loss scale overflow! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            "100% 125/125 [00:41<00:00,  3.02it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 43.1738, 'train_samples_per_second': 2.895, 'epoch': 1.0}\n",
            "100% 125/125 [00:41<00:00,  3.04it/s]\n",
            "Saving model checkpoint to output_dir\n",
            "Configuration saved in output_dir/config.json\n",
            "Model weights saved in output_dir/pytorch_model.bin\n",
            "tokenizer config file saved in output_dir/tokenizer_config.json\n",
            "Special tokens file saved in output_dir/special_tokens_map.json\n",
            "Copy vocab file to output_dir/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =     1.0\n",
            "  init_mem_cpu_alloc_delta   =     2MB\n",
            "  init_mem_cpu_peaked_delta  =     0MB\n",
            "  init_mem_gpu_alloc_delta   =     0MB\n",
            "  init_mem_gpu_peaked_delta  =     0MB\n",
            "  train_mem_cpu_alloc_delta  =    84MB\n",
            "  train_mem_cpu_peaked_delta =     0MB\n",
            "  train_mem_gpu_alloc_delta  =   148MB\n",
            "  train_mem_gpu_peaked_delta =  4322MB\n",
            "  train_runtime              = 43.1738\n",
            "  train_samples              =    2000\n",
            "  train_samples_per_second   =   2.895\n",
            "03/25/2021 03:14:22 - INFO - __main__ -   *** Evaluate ***\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 500\n",
            "  Batch size = 16\n",
            "100% 32/32 [01:02<00:00,  1.95s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                     =     1.0\n",
            "  eval_bleu                 = 23.8371\n",
            "  eval_gen_len              =  39.384\n",
            "  eval_loss                 =  3.3829\n",
            "  eval_mem_cpu_alloc_delta  =     1MB\n",
            "  eval_mem_cpu_peaked_delta =     2MB\n",
            "  eval_mem_gpu_alloc_delta  =     0MB\n",
            "  eval_mem_gpu_peaked_delta =   376MB\n",
            "  eval_runtime              = 64.2663\n",
            "  eval_samples              =     500\n",
            "  eval_samples_per_second   =    7.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSlYvQWLwblN"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}