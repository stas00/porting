{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DeepSpeed on colab CLI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9mmhJzcgHy1",
        "outputId": "ac00301a-32d1-41e4-a458-8eb050735e08"
      },
      "source": [
        "# need to match the system-wide installed cuda-11 for deepspeed to compile\n",
        "# so install the matching pytorch\n",
        "\n",
        "!pip install torch==1.7.1+cu110  -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu110\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu110/torch-1.7.1%2Bcu110-cp37-cp37m-linux_x86_64.whl (1156.8MB)\n",
            "\u001b[K     |███████████████████████         | 834.1MB 63.9MB/s eta 0:00:06tcmalloc: large alloc 1147494400 bytes == 0x559d60534000 @  0x7fa4a80ce615 0x559d27c3b06c 0x559d27d1aeba 0x559d27c3de8d 0x559d27d2f99d 0x559d27cb1fe9 0x559d27cacb0e 0x559d27c3f77a 0x559d27cb1e50 0x559d27cacb0e 0x559d27c3f77a 0x559d27cae86a 0x559d27d307c6 0x559d27cadee2 0x559d27d307c6 0x559d27cadee2 0x559d27d307c6 0x559d27cadee2 0x559d27d307c6 0x559d27db2431 0x559d27d13049 0x559d27c7dc84 0x559d27c3e8e9 0x559d27cb2ade 0x559d27c3f69a 0x559d27cada45 0x559d27cace0d 0x559d27c3f77a 0x559d27cada45 0x559d27c3f69a 0x559d27cada45\n",
            "\u001b[K     |█████████████████████████████▏  | 1055.7MB 1.2MB/s eta 0:01:25tcmalloc: large alloc 1434370048 bytes == 0x559da4b8a000 @  0x7fa4a80ce615 0x559d27c3b06c 0x559d27d1aeba 0x559d27c3de8d 0x559d27d2f99d 0x559d27cb1fe9 0x559d27cacb0e 0x559d27c3f77a 0x559d27cb1e50 0x559d27cacb0e 0x559d27c3f77a 0x559d27cae86a 0x559d27d307c6 0x559d27cadee2 0x559d27d307c6 0x559d27cadee2 0x559d27d307c6 0x559d27cadee2 0x559d27d307c6 0x559d27db2431 0x559d27d13049 0x559d27c7dc84 0x559d27c3e8e9 0x559d27cb2ade 0x559d27c3f69a 0x559d27cada45 0x559d27cace0d 0x559d27c3f77a 0x559d27cada45 0x559d27c3f69a 0x559d27cada45\n",
            "\u001b[K     |████████████████████████████████| 1156.7MB 1.2MB/s eta 0:00:01tcmalloc: large alloc 1445945344 bytes == 0x559dfa376000 @  0x7fa4a80ce615 0x559d27c3b06c 0x559d27d1aeba 0x559d27c3de8d 0x559d27d2f99d 0x559d27cb1fe9 0x559d27cacb0e 0x559d27c3f77a 0x559d27cadc9e 0x559d27cacb0e 0x559d27c3f77a 0x559d27cadc9e 0x559d27cacb0e 0x559d27c3f77a 0x559d27cadc9e 0x559d27cacb0e 0x559d27c3f77a 0x559d27cadc9e 0x559d27cacb0e 0x559d27c3f77a 0x559d27cadc9e 0x559d27c3f69a 0x559d27cadc9e 0x559d27cacb0e 0x559d27c3f77a 0x559d27cae86a 0x559d27cacb0e 0x559d27c3f77a 0x559d27cae86a 0x559d27cacb0e 0x559d27c3fe11\n",
            "\u001b[K     |████████████████████████████████| 1156.8MB 15kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1+cu110) (3.7.4.3)\n",
            "Installing collected packages: torch\n",
            "  Found existing installation: torch 1.7.1+cu101\n",
            "    Uninstalling torch-1.7.1+cu101:\n",
            "      Successfully uninstalled torch-1.7.1+cu101\n",
            "Successfully installed torch-1.7.1+cu110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aNVOVxab2Ds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99aff746-e906-4289-c709-412ba76f40b0"
      },
      "source": [
        "!pip install deepspeed"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeed\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/bd/b2b544ca1286252e9a559b1508e64d0d61af7a73b6bf6737568858128e11/deepspeed-0.3.10.tar.gz (281kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.7.1+cu110)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from deepspeed) (0.8.2+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deepspeed) (4.41.1)\n",
            "Collecting tensorboardX==1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/12/dcaf67e1312475b26db9e45e7bb6f32b540671a9ee120b3a72d9e09bc517/tensorboardX-1.8-py2.py3-none-any.whl (216kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 9.0MB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/de/393468f2a37fc2c1dc3a06afc37775e27fde2d16845424141d4da62c686d/ninja-1.10.0.post2-py3-none-manylinux1_x86_64.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 8.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deepspeed) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->deepspeed) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->deepspeed) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX==1.8->deepspeed) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.2.0->tensorboardX==1.8->deepspeed) (54.0.0)\n",
            "Building wheels for collected packages: deepspeed\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.3.10-cp37-none-any.whl size=272625 sha256=452186cc24c832b2193de6b7148e2ebd11b03ac7caba3a941fbfe6ba54faf81d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/3c/9c/39a16330874a2c55f61fe2c501e120258975d509177ffdcda7\n",
            "Successfully built deepspeed\n",
            "Installing collected packages: tensorboardX, ninja, deepspeed\n",
            "Successfully installed deepspeed-0.3.10 ninja-1.10.0.post2 tensorboardX-1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZQAIH70Yykn",
        "outputId": "4ce4c49f-ecc8-4f86-e8a4-fc23b63963de"
      },
      "source": [
        "%%bash\r\n",
        "git clone https://github.com/huggingface/transformers\r\n",
        "cd transformers\r\n",
        "pip install -e .\r\n",
        "pip install -r examples/_tests_requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/transformers\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "    Preparing wheel metadata: started\n",
            "    Preparing wheel metadata: finished with status 'done'\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0.dev0) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0.dev0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.0.dev0) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0.dev0) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py): started\n",
            "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=f99cc83a832b9a8908f8735131521efc88df980b222806beffc870378b1332a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "  Running setup.py develop for transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 1)) (2.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 2)) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 4)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/7f/4fd83db8570288c3899d8e57666c2841403c15659f3d792a3cb8dc1c6689/sacrebleu-1.5.0-py3-none-any.whl (65kB)\n",
            "Collecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/56/a81022436c08b9405a5247b71635394d44fe7e1dbedc4b28c740e09c2840/rouge_score-0.0.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 7)) (4.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 8)) (3.2.2)\n",
            "Collecting git-python==1.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/de/0cc6353a45cdb1e137cffac5383097b300cc578e2e1133eeb847e23a1394/git_python-1.0.3-py2.py3-none-any.whl\n",
            "Collecting faiss-cpu\n",
            "  Downloading https://files.pythonhosted.org/packages/48/0c/efd43c4feac172867409f38f07949c36602355ec7196749d10f905d09228/faiss_cpu-1.7.0-cp37-cp37m-manylinux2014_x86_64.whl (8.1MB)\n",
            "Collecting streamlit\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/6c/c03f12bbbd8367152897c3b3269f87b717b3e7b834b44d15aae345727375/streamlit-0.77.0-py2.py3-none-any.whl (7.5MB)\n",
            "Collecting elasticsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/72/68/76c5d46cc6a48fddb759f585bc8728caa11bfc9b812ce6705fc5f99beab2/elasticsearch-7.11.0-py2.py3-none-any.whl (325kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 13)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 14)) (1.1.5)\n",
            "Collecting datasets>=1.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3e/73/742d17d8a9a1c639132affccc9250f0743e484cbf263ede6ddcbe34ef212/datasets-1.4.1-py3-none-any.whl (186kB)\n",
            "Collecting fire\n",
            "  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 17)) (3.6.4)\n",
            "Collecting conllu\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/be/be6959c3ff2dbfdd87de4be0ccdff577835b5d08b1d25bf7fd4aaf0d7add/conllu-4.4-py2.py3-none-any.whl\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from -r examples/_tests_requirements.txt (line 20)) (3.12.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (54.0.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.27.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (3.3.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.36.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r examples/_tests_requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r examples/_tests_requirements.txt (line 2)) (1.4.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/82/22/e684c9e2e59b561dbe36538852e81849122c666c423448e3a5c99362c228/portalocker-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.3.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.1.5)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (0.28.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (2.3)\n",
            "Requirement already satisfied: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (5.1.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (20.3.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r examples/_tests_requirements.txt (line 8)) (0.10.0)\n",
            "Collecting gitpython\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (0.8.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading https://files.pythonhosted.org/packages/1c/3f/8f04ae0c22d82ec7bec7fcc03270a142f637e362bbd285f7daeeda24fbef/pydeck-0.6.1-py2.py3-none-any.whl (4.6MB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (7.1.2)\n",
            "Collecting blinker\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (1.5.1)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (20.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (7.0.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (4.2.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (0.10.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (4.1.0)\n",
            "Collecting watchdog; platform_system != \"Darwin\"\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/ba/a36ca5b4e75649a002f06531862467b3eb5c768caa23d6d88b921fe238d8/watchdog-2.0.2-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "Collecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit->-r examples/_tests_requirements.txt (line 11)) (5.1.1)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r examples/_tests_requirements.txt (line 12)) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elasticsearch->-r examples/_tests_requirements.txt (line 12)) (2020.12.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r examples/_tests_requirements.txt (line 14)) (2018.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (0.70.11.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (3.7.0)\n",
            "Collecting huggingface-hub==0.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/93/7cb0755c62c36cdadc70c79a95681df685b52cbaf76c724facb6ecac3272/huggingface_hub-0.0.2-py3-none-any.whl\n",
            "Collecting xxhash\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "Collecting fsspec\n",
            "  Downloading https://files.pythonhosted.org/packages/91/0d/a6bfee0ddf47b254286b9bd574e6f50978c69897647ae15b14230711806e/fsspec-0.8.7-py3-none-any.whl (103kB)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (8.7.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (1.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r examples/_tests_requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r examples/_tests_requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r examples/_tests_requirements.txt (line 1)) (4.7.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r examples/_tests_requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard->-r examples/_tests_requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (1.52.0)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets->-r examples/_tests_requirements.txt (line 7)) (3.4.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n",
            "  Downloading https://files.pythonhosted.org/packages/56/95/3a670c8b2c2370bd8631c313f42e60983b3113ffec4035940592252bd6d5/ipykernel-5.5.0-py3-none-any.whl (120kB)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (7.6.3)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (2.11.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.0.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.11.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.3)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit->-r examples/_tests_requirements.txt (line 11)) (4.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets>=1.1.3->-r examples/_tests_requirements.txt (line 15)) (3.0.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r examples/_tests_requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->-r examples/_tests_requirements.txt (line 1)) (0.4.8)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.3.5)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.1.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (22.0.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.7.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=5.0.0->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.9.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (1.4.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.4.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (3.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->-r examples/_tests_requirements.txt (line 11)) (0.5.1)\n",
            "Building wheels for collected packages: seqeval, fire, blinker\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=f8051ac777411c02eb0ba0d14e5f1a58c92dffa00417fbb9fa3dae7e390636ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for fire (setup.py): started\n",
            "  Building wheel for fire (setup.py): finished with status 'done'\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=4dfce74fa0fdd1ea7149f77029e45576ee97aea1543ce4a567eabbf4d9a194f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "  Building wheel for blinker (setup.py): started\n",
            "  Building wheel for blinker (setup.py): finished with status 'done'\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=a85f28d8467464587ad9e1dc32af0089815f8180ce785d5c9506ab2c10b5ada5\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built seqeval fire blinker\n",
            "Installing collected packages: seqeval, portalocker, sacrebleu, rouge-score, smmap, gitdb, gitpython, git-python, faiss-cpu, ipykernel, pydeck, blinker, watchdog, base58, validators, streamlit, elasticsearch, huggingface-hub, xxhash, fsspec, datasets, fire, conllu, sentencepiece\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "Successfully installed base58-2.1.0 blinker-1.4 conllu-4.4 datasets-1.4.1 elasticsearch-7.11.0 faiss-cpu-1.7.0 fire-0.4.0 fsspec-0.8.7 git-python-1.0.3 gitdb-4.0.5 gitpython-3.1.14 huggingface-hub-0.0.2 ipykernel-5.5.0 portalocker-2.2.1 pydeck-0.6.1 rouge-score-0.0.4 sacrebleu-1.5.0 sentencepiece-0.1.95 seqeval-1.2.2 smmap-3.0.5 streamlit-0.77.0 validators-0.18.2 watchdog-2.0.2 xxhash-2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "ERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.0 which is incompatible.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYG2EDAQdFxt"
      },
      "source": [
        "%%bash\n",
        "\n",
        "cd transformers\n",
        "\n",
        "cat <<'EOT' > ds_config.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": true,\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "       \"allgather_partitions\": true,\n",
        "       \"allgather_bucket_size\": 2e8,\n",
        "       \"reduce_scatter\": true,\n",
        "       \"reduce_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"cpu_offload\": true\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"Adam\",\n",
        "        \"params\": {\n",
        "            \"adam_w_mode\": true,\n",
        "            \"lr\": 3e-5,\n",
        "            \"betas\": [ 0.9, 0.999 ],\n",
        "            \"eps\": 1e-8,\n",
        "            \"weight_decay\": 3e-7\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": 0,\n",
        "            \"warmup_max_lr\": 3e-5,\n",
        "            \"warmup_num_steps\": 500\n",
        "        }\n",
        "    }\n",
        "}\n",
        "EOT\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJKdum6zdxLE",
        "outputId": "a776aca7-b6cc-41b3-e869-02cb88c462b9"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x  1 root root 4096 Mar  1 14:35 sample_data\n",
            "drwxr-xr-x 15 root root 4096 Mar  4 16:30 transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghNfx0nNZSfq",
        "outputId": "1ab1c5f8-f7a5-4992-b7c0-b8ba1dd8069f"
      },
      "source": [
        "\r\n",
        "!cd transformers; export BS=16; rm -r output_dir; PYTHONPATH=src USE_TF=0 CUDA_VISIBLE_DEVICES=0 deepspeed --num_gpus=1 examples/seq2seq/run_seq2seq.py --model_name_or_path google/mt5-small \\\r\n",
        "--output_dir output_dir --adam_eps 1e-06  --evaluation_strategy=steps  --do_train --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 \\\r\n",
        "--max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir  --per_device_train_batch_size $BS --predict_with_generate --sortish_sampler \\\r\n",
        "--val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_val_samples 500 \\\r\n",
        "--task translation_en_to_ro  --dataset_name wmt16 --dataset_config ro-en --source_prefix \"translate English to Romanian: \" --deepspeed  ds_config.json --fp16\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'output_dir': No such file or directory\n",
            "[2021-03-04 16:30:56,136] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
            "[2021-03-04 16:30:56,153] [INFO] [runner.py:355:main] cmd = /usr/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMF19 --master_addr=127.0.0.1 --master_port=29500 examples/seq2seq/run_seq2seq.py --model_name_or_path google/mt5-small --output_dir output_dir --adam_eps 1e-06 --evaluation_strategy=steps --do_train --label_smoothing 0.1 --learning_rate 3e-5 --logging_first_step --logging_steps 1000 --max_source_length 128 --max_target_length 128 --num_train_epochs 1 --overwrite_output_dir --per_device_train_batch_size 16 --predict_with_generate --sortish_sampler --val_max_target_length 128 --warmup_steps 500 --max_train_samples 2000 --max_val_samples 500 --task translation_en_to_ro --dataset_name wmt16 --dataset_config ro-en --source_prefix translate English to Romanian:  --deepspeed ds_config.json --fp16\n",
            "[2021-03-04 16:30:57,285] [INFO] [launch.py:71:main] 0 NCCL_VERSION 2.7.8\n",
            "[2021-03-04 16:30:57,285] [INFO] [launch.py:78:main] WORLD INFO DICT: {'localhost': [0]}\n",
            "[2021-03-04 16:30:57,285] [INFO] [launch.py:87:main] nnodes=1, num_local_procs=1, node_rank=0\n",
            "[2021-03-04 16:30:57,285] [INFO] [launch.py:99:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0]})\n",
            "[2021-03-04 16:30:57,285] [INFO] [launch.py:100:main] dist_world_size=1\n",
            "[2021-03-04 16:30:57,285] [INFO] [launch.py:103:main] Setting CUDA_VISIBLE_DEVICES=0\n",
            "[2021-03-04 16:31:02,225] [INFO] [distributed.py:40:init_distributed] Initializing torch distributed with backend: nccl\n",
            "03/04/2021 16:31:04 - WARNING - __main__ -   Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
            "03/04/2021 16:31:04 - INFO - __main__ -   Training/evaluation parameters Seq2SeqTrainingArguments(output_dir='output_dir', overwrite_output_dir=True, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=3e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=500, logging_dir='runs/Mar04_16-31-02_021d74528ddc', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=True, logging_steps=1000, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=True, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=0, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, dataloader_num_workers=0, past_index=-1, run_name='output_dir', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed='ds_config.json', label_smoothing_factor=0.1, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, sortish_sampler=True, predict_with_generate=True)\n",
            "Downloading: 2.81kB [00:00, 2.48MB/s]       \n",
            "Downloading: 3.19kB [00:00, 3.05MB/s]       \n",
            "Downloading: 41.1kB [00:00, 23.7MB/s]       \n",
            "Downloading and preparing dataset wmt16/ro-en (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f...\n",
            "Downloading: 100% 225M/225M [00:03<00:00, 62.0MB/s]\n",
            "Downloading: 100% 23.5M/23.5M [00:03<00:00, 5.87MB/s]\n",
            "Downloading: 100% 38.7M/38.7M [00:00<00:00, 59.6MB/s]\n",
            "Dataset wmt16 downloaded and prepared to /root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9dc00622c30446e99c4c63d12a484ea4fb653f2f37c867d6edcec839d7eae50f. Subsequent calls will reuse this data.\n",
            "https://huggingface.co/google/mt5-small/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpekmvlz1e\n",
            "Downloading: 100% 553/553 [00:00<00:00, 413kB/s]\n",
            "storing https://huggingface.co/google/mt5-small/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "creating metadata file for /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "Model config MT5Config {\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/google/mt5-small/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/97693496c1a0cae463bd18428187f9e9924d2dfbadaa46e4d468634a0fc95a41.dadce13f8f85f4825168354a04675d4b177749f8f11b167e87676777695d4fe4\n",
            "Model config MT5Config {\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 1024,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 512,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 8,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.4.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "Model name 'google/mt5-small' not found in model shortcut name list (t5-small, t5-base, t5-large, t5-3b, t5-11b). Assuming 'google/mt5-small' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "https://huggingface.co/google/mt5-small/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphv9gyb7i\n",
            "Downloading: 100% 4.31M/4.31M [00:00<00:00, 17.6MB/s]\n",
            "storing https://huggingface.co/google/mt5-small/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
            "creating metadata file for /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
            "https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpf41tu4ce\n",
            "Downloading: 100% 99.0/99.0 [00:00<00:00, 77.8kB/s]\n",
            "storing https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
            "creating metadata file for /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
            "https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0qm5excp\n",
            "Downloading: 100% 82.0/82.0 [00:00<00:00, 67.8kB/s]\n",
            "storing https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
            "creating metadata file for /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
            "loading file https://huggingface.co/google/mt5-small/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/37d0f67f084f8c5fc5589e0bba5ff3c6307af833bb0b7f4eb33fbfd8d4038a9d.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\n",
            "loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/google/mt5-small/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/google/mt5-small/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/685ac0ca8568ec593a48b61b0a3c272beee9bc194a3c7241d15dcadb5f875e53.f76030f3ec1b96a8199b2593390c610e76ca8028ef3d24680000619ffb646276\n",
            "loading file https://huggingface.co/google/mt5-small/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/6a9e52d6dd21568e37b65fc180ada927968e8f7124f0acd6efcaf90cd2e0f4bb.4b81e5d952ad810ca1de2b3e362b9a26a5cc77b4b75daf20caf69fb838751c32\n",
            "https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp56brm8y7\n",
            "Downloading: 100% 1.20G/1.20G [00:18<00:00, 63.9MB/s]\n",
            "storing https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
            "creating metadata file for /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
            "loading weights file https://huggingface.co/google/mt5-small/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8e7b2a80ddcb5611b27d8c89e1e8e33a947e105415051402a22b9c8d7d1caeb0.e22331f3a065b885b30ae3dd1ff11ccaf7fbc444485f6eb07ef5e0138bca8b70\n",
            "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
            "100% 2/2 [00:00<00:00, 17.19ba/s]\n",
            "Downloading: 5.02kB [00:00, 3.34MB/s]       \n",
            "Using amp fp16 backend\n",
            "Keeping the `optimizer` config from ds_config.json intact, ignoring any optimizer-specific cl args\n",
            "Keeping the `scheduler` config from ds_config.json intact, ignoring any scheduler-specific cl args\n",
            "Keeping the `fp16` config from ds_config.json intact, ignoring any fp16-specific cl args\n",
            "[2021-03-04 16:32:11,033] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed info: version=0.3.10, git-hash=unknown, git-branch=unknown\n",
            "[2021-03-04 16:32:11,451] [INFO] [engine.py:72:_initialize_parameter_parallel_groups] data_parallel_size: 1, parameter_parallel_size: 1\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/cpu_adam...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[1/3] /usr/local/cuda/bin/nvcc -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_60,code=sm_60 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++14 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_60,code=sm_60 -gencode=arch=compute_60,code=compute_60 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o \n",
            "[2/3] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/includes -I/usr/local/cuda/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++14 -L/usr/local/cuda/lib64 -lcudart -lcublas -g -Wno-reorder -march=native -fopenmp -D__AVX256__ -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o \n",
            "[3/3] c++ cpu_adam.o custom_cuda_kernel.cuda.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cpu_adam.so\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 25.022990465164185 seconds\n",
            "Adam Optimizer #0 is created with AVX2 arithmetic capability.\n",
            "Config: alpha=0.000030, betas=(0.900000, 0.999000), weight_decay=0.000000, adam_w=1\n",
            "[2021-03-04 16:32:41,377] [INFO] [engine.py:518:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
            "[2021-03-04 16:32:41,377] [INFO] [engine.py:521:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: [0.9, 0.999]\n",
            "    bias_correction: True\n",
            "    eps: 1e-08\n",
            "    lr: 3e-05\n",
            "    weight_decay: 3e-07\n",
            ")\n",
            "Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2021-03-04 16:32:41,377] [INFO] [engine.py:638:_configure_zero_optimizer] Creating fp16 ZeRO stage 2 optimizer\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Creating extension directory /root/.cache/torch_extensions/utils...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /usr/local/lib/python3.7/dist-packages/torch/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/torch/csrc/api/include -isystem /usr/local/lib/python3.7/dist-packages/torch/include/TH -isystem /usr/local/lib/python3.7/dist-packages/torch/include/THC -isystem /usr/include/python3.7m -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /usr/local/lib/python3.7/dist-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o \n",
            "[2/2] c++ flatten_unflatten.o -shared -L/usr/local/lib/python3.7/dist-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 13.71750521659851 seconds\n",
            "[2021-03-04 16:32:55,095] [INFO] [stage2.py:130:__init__] Reduce bucket size 200000000.0\n",
            "[2021-03-04 16:32:55,096] [INFO] [stage2.py:131:__init__] Allgather bucket size 200000000.0\n",
            "[2021-03-04 16:32:55,096] [INFO] [stage2.py:132:__init__] CPU Offload: True\n",
            "tcmalloc: large alloc 1200709632 bytes == 0x55b0269fe000 @  0x7f27dedd8b6b 0x7f27dedf8379 0x7f26f85b474e 0x7f26f85b67b6 0x7f278413ae43 0x7f2783b257ff 0x7f2783e3cbdc 0x7f2783de824b 0x7f2783e07065 0x7f2783de2a7b 0x7f2783de824b 0x7f2783e07065 0x7f2783ed11ee 0x7f27853e3d9e 0x7f2783de824b 0x7f2783e07065 0x7f2783de2a7b 0x7f2783de824b 0x7f2783e07065 0x7f2783ed11ee 0x7f2783b11730 0x7f278403e71a 0x7f2783690081 0x7f278412ad66 0x7f278410e900 0x7f2793a7866d 0x7f2793b83313 0x7f2793b8f200 0x55af211ad050 0x55af2129e99d 0x55af21220fe9\n",
            "group 0 param 0 = 300176768\n",
            "[2021-03-04 16:32:58,267] [INFO] [stage2.py:399:__init__] optimizer state initialized\n",
            "[2021-03-04 16:32:58,267] [INFO] [engine.py:551:_configure_optimizer] DeepSpeed Final Optimizer = <deepspeed.runtime.zero.stage2.FP16_DeepSpeedZeroOptimizer object at 0x7f26a8f94290>\n",
            "[2021-03-04 16:32:58,267] [INFO] [engine.py:382:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-03-04 16:32:58,267] [INFO] [logging.py:60:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f26a9626a90>\n",
            "[2021-03-04 16:32:58,268] [INFO] [logging.py:60:log_dist] [Rank 0] step=0, skipped=0, lr=[3e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:705:print] DeepSpeedEngine configuration:\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   activation_checkpointing_config  <deepspeed.runtime.activation_checkpointing.config.DeepSpeedActivationCheckpointingConfig object at 0x7f26a99b05d0>\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   allreduce_always_fp32 ........ False\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   amp_enabled .................. False\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   amp_params ................... False\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   disable_allgather ............ False\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   dump_state ................... False\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   dynamic_loss_scale_args ...... {'init_scale': 4294967296, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   elasticity_enabled ........... False\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   fp16_enabled ................. True\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   global_rank .................. 0\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   gradient_accumulation_steps .. 1\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   gradient_clipping ............ 1.0\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   initial_dynamic_scale ........ 4294967296\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   loss_scale ................... 0\n",
            "[2021-03-04 16:32:58,268] [INFO] [config.py:709:print]   memory_breakdown ............. False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   optimizer_name ............... adam\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   optimizer_params ............. {'lr': 3e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   pld_enabled .................. False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   pld_params ................... False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   prescale_gradients ........... False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   scheduler_name ............... WarmupLR\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 3e-05, 'warmup_num_steps': 500}\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   sparse_attention ............. None\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   sparse_gradients_enabled ..... False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   steps_per_print .............. 10\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   tensorboard_enabled .......... False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   tensorboard_output_path ...... \n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   train_batch_size ............. 16\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   train_micro_batch_size_per_gpu  16\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   wall_clock_breakdown ......... False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   world_size ................... 1\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   zero_allow_untested_optimizer  False\n",
            "[2021-03-04 16:32:58,269] [INFO] [config.py:709:print]   zero_config .................. {\n",
            "    \"allgather_bucket_size\": 200000000.0,\n",
            "    \"allgather_partitions\": true,\n",
            "    \"contiguous_gradients\": true,\n",
            "    \"cpu_offload\": true,\n",
            "    \"elastic_checkpoint\": true,\n",
            "    \"load_from_fp32_weights\": true,\n",
            "    \"overlap_comm\": true,\n",
            "    \"reduce_bucket_size\": 200000000.0,\n",
            "    \"reduce_scatter\": true,\n",
            "    \"stage\": 2\n",
            "}\n",
            "[2021-03-04 16:32:58,270] [INFO] [config.py:709:print]   zero_enabled ................. True\n",
            "[2021-03-04 16:32:58,270] [INFO] [config.py:709:print]   zero_optimization_stage ...... 2\n",
            "[2021-03-04 16:32:58,270] [INFO] [config.py:715:print]   json = {\n",
            "    \"fp16\":{\n",
            "        \"enabled\":true,\n",
            "        \"hysteresis\":2,\n",
            "        \"loss_scale\":0,\n",
            "        \"loss_scale_window\":1000,\n",
            "        \"min_loss_scale\":1\n",
            "    },\n",
            "    \"gradient_accumulation_steps\":1,\n",
            "    \"gradient_clipping\":1.0,\n",
            "    \"optimizer\":{\n",
            "        \"params\":{\n",
            "            \"betas\":[\n",
            "                0.9,\n",
            "                0.999\n",
            "            ],\n",
            "            \"eps\":1e-08,\n",
            "            \"lr\":3e-05,\n",
            "            \"weight_decay\":3e-07\n",
            "        },\n",
            "        \"type\":\"Adam\"\n",
            "    },\n",
            "    \"scheduler\":{\n",
            "        \"params\":{\n",
            "            \"warmup_max_lr\":3e-05,\n",
            "            \"warmup_min_lr\":0,\n",
            "            \"warmup_num_steps\":500\n",
            "        },\n",
            "        \"type\":\"WarmupLR\"\n",
            "    },\n",
            "    \"train_micro_batch_size_per_gpu\":16,\n",
            "    \"zero_optimization\":{\n",
            "        \"allgather_bucket_size\":200000000.0,\n",
            "        \"allgather_partitions\":true,\n",
            "        \"contiguous_gradients\":true,\n",
            "        \"cpu_offload\":true,\n",
            "        \"overlap_comm\":true,\n",
            "        \"reduce_bucket_size\":200000000.0,\n",
            "        \"reduce_scatter\":true,\n",
            "        \"stage\":2\n",
            "    }\n",
            "}\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.0006463527679443359 seconds\n",
            "***** Running training *****\n",
            "  Num examples = 2000\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 125\n",
            "2021-03-04 16:32:58.484979: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "  0% 0/125 [00:00<?, ?it/s][2021-03-04 16:33:06,476] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 4294967296\n",
            "  1% 1/125 [00:05<10:56,  5.29s/it]tried to get lr value before scheduler/optimizer started stepping, returning lr=0\n",
            "{'loss': nan, 'learning_rate': 0, 'epoch': 0.01}\n",
            "  1% 1/125 [00:05<10:56,  5.29s/it][2021-03-04 16:33:06,823] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648.0\n",
            "  2% 2/125 [00:05<07:48,  3.81s/it][2021-03-04 16:33:07,176] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648.0, reducing to 1073741824.0\n",
            "  2% 3/125 [00:05<05:38,  2.77s/it][2021-03-04 16:33:07,613] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824.0, reducing to 536870912.0\n",
            "  3% 4/125 [00:06<04:10,  2.07s/it][2021-03-04 16:33:07,989] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912.0, reducing to 268435456.0\n",
            "  4% 5/125 [00:06<03:07,  1.56s/it][2021-03-04 16:33:08,354] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456.0, reducing to 134217728.0\n",
            "  5% 6/125 [00:07<02:23,  1.20s/it][2021-03-04 16:33:08,689] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728.0, reducing to 67108864.0\n",
            "  6% 7/125 [00:07<01:51,  1.06it/s][2021-03-04 16:33:09,032] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864.0, reducing to 33554432.0\n",
            "  6% 8/125 [00:07<01:29,  1.31it/s][2021-03-04 16:33:09,375] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432.0, reducing to 16777216.0\n",
            "  7% 9/125 [00:08<01:13,  1.57it/s][2021-03-04 16:33:09,726] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216.0, reducing to 8388608.0\n",
            "[2021-03-04 16:33:09,727] [INFO] [timer.py:166:stop] 0/10, SamplesPerSec=45.18859808603419\n",
            "  8% 10/125 [00:08<01:03,  1.81it/s][2021-03-04 16:33:10,083] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608.0, reducing to 4194304.0\n",
            "  9% 11/125 [00:08<00:56,  2.03it/s][2021-03-04 16:33:10,513] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304.0, reducing to 2097152.0\n",
            " 10% 12/125 [00:09<00:53,  2.11it/s][2021-03-04 16:33:10,858] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152.0, reducing to 1048576.0\n",
            " 10% 13/125 [00:09<00:48,  2.30it/s][2021-03-04 16:33:11,168] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0\n",
            " 11% 14/125 [00:09<00:44,  2.51it/s][2021-03-04 16:33:11,510] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0\n",
            " 12% 15/125 [00:10<00:41,  2.62it/s][2021-03-04 16:33:11,814] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0\n",
            " 13% 16/125 [00:10<00:39,  2.79it/s][2021-03-04 16:33:12,169] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0\n",
            " 14% 17/125 [00:10<00:38,  2.80it/s][2021-03-04 16:33:12,526] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0\n",
            " 14% 18/125 [00:11<00:38,  2.80it/s][2021-03-04 16:33:12,830] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0\n",
            " 15% 19/125 [00:11<00:36,  2.93it/s][2021-03-04 16:33:13,211] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
            "[2021-03-04 16:33:13,212] [INFO] [timer.py:166:stop] 0/20, SamplesPerSec=46.207672880340425\n",
            " 16% 20/125 [00:12<00:37,  2.83it/s][2021-03-04 16:33:13,550] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
            " 17% 21/125 [00:12<00:36,  2.87it/s][2021-03-04 16:33:13,902] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
            " 18% 22/125 [00:12<00:36,  2.86it/s][2021-03-04 16:33:14,255] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0\n",
            " 18% 23/125 [00:13<00:35,  2.85it/s][2021-03-04 16:33:14,571] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024.0, reducing to 512.0\n",
            " 19% 24/125 [00:13<00:34,  2.94it/s][2021-03-04 16:33:14,924] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512.0, reducing to 256.0\n",
            " 20% 25/125 [00:13<00:34,  2.91it/s][2021-03-04 16:33:15,257] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256.0, reducing to 128.0\n",
            " 21% 26/125 [00:14<00:33,  2.93it/s][2021-03-04 16:33:15,608] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128.0, reducing to 64.0\n",
            " 22% 27/125 [00:14<00:33,  2.91it/s][2021-03-04 16:33:15,992] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64.0, reducing to 32.0\n",
            " 22% 28/125 [00:14<00:34,  2.81it/s]rank=0 time (ms) | optimizer_gradients: 61.28 | optimizer_step: 411.16 | optimizer_allgather: 5.16\n",
            " 23% 29/125 [00:15<00:47,  2.00it/s][2021-03-04 16:33:17,176] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32.0, reducing to 16.0\n",
            "[2021-03-04 16:33:17,178] [INFO] [timer.py:166:stop] 0/30, SamplesPerSec=44.306742400722015\n",
            " 24% 30/125 [00:15<00:43,  2.20it/s][2021-03-04 16:33:17,484] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16.0, reducing to 8.0\n",
            " 25% 31/125 [00:16<00:38,  2.44it/s]rank=0 time (ms) | optimizer_gradients: 67.48 | optimizer_step: 401.04 | optimizer_allgather: 4.91\n",
            " 26% 32/125 [00:17<00:48,  1.91it/s]rank=0 time (ms) | optimizer_gradients: 59.60 | optimizer_step: 402.47 | optimizer_allgather: 4.86\n",
            " 26% 33/125 [00:17<00:55,  1.67it/s]rank=0 time (ms) | optimizer_gradients: 61.58 | optimizer_step: 399.40 | optimizer_allgather: 4.88\n",
            " 27% 34/125 [00:18<01:00,  1.50it/s]rank=0 time (ms) | optimizer_gradients: 61.82 | optimizer_step: 402.82 | optimizer_allgather: 4.88\n",
            " 28% 35/125 [00:19<01:04,  1.40it/s]rank=0 time (ms) | optimizer_gradients: 62.24 | optimizer_step: 398.11 | optimizer_allgather: 4.86\n",
            " 29% 36/125 [00:20<01:05,  1.35it/s]rank=0 time (ms) | optimizer_gradients: 61.29 | optimizer_step: 401.78 | optimizer_allgather: 4.86\n",
            " 30% 37/125 [00:21<01:07,  1.31it/s]rank=0 time (ms) | optimizer_gradients: 59.71 | optimizer_step: 397.29 | optimizer_allgather: 4.89\n",
            " 30% 38/125 [00:21<01:08,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 60.85 | optimizer_step: 407.30 | optimizer_allgather: 4.86\n",
            " 31% 39/125 [00:22<01:07,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 60.54 | optimizer_step: 399.19 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:33:24,789] [INFO] [logging.py:60:log_dist] [Rank 0] step=40, skipped=30, lr=[1.1115351393977566e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:33:24,790] [INFO] [timer.py:166:stop] 0/40, SamplesPerSec=34.48716972344762\n",
            " 32% 40/125 [00:23<01:08,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 61.92 | optimizer_step: 399.49 | optimizer_allgather: 4.88\n",
            " 33% 41/125 [00:24<01:08,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 60.37 | optimizer_step: 397.62 | optimizer_allgather: 4.88\n",
            " 34% 42/125 [00:25<01:08,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 61.00 | optimizer_step: 401.40 | optimizer_allgather: 4.87\n",
            " 34% 43/125 [00:26<01:06,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 62.90 | optimizer_step: 402.82 | optimizer_allgather: 4.86\n",
            " 35% 44/125 [00:26<01:06,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 60.17 | optimizer_step: 405.52 | optimizer_allgather: 4.88\n",
            " 36% 45/125 [00:27<01:04,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 60.48 | optimizer_step: 401.05 | optimizer_allgather: 4.91\n",
            " 37% 46/125 [00:28<01:03,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 62.96 | optimizer_step: 404.77 | optimizer_allgather: 4.88\n",
            " 38% 47/125 [00:29<01:03,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 60.95 | optimizer_step: 399.31 | optimizer_allgather: 4.89\n",
            " 38% 48/125 [00:30<01:02,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 68.78 | optimizer_step: 418.17 | optimizer_allgather: 4.88\n",
            " 39% 49/125 [00:30<01:01,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 61.39 | optimizer_step: 398.00 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:33:32,949] [INFO] [logging.py:60:log_dist] [Rank 0] step=50, skipped=30, lr=[1.446140557591026e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:33:32,949] [INFO] [timer.py:166:stop] 0/50, SamplesPerSec=29.88005737793884\n",
            " 40% 50/125 [00:31<01:00,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 61.39 | optimizer_step: 402.04 | optimizer_allgather: 4.86\n",
            " 41% 51/125 [00:32<01:00,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 62.28 | optimizer_step: 399.64 | optimizer_allgather: 4.87\n",
            " 42% 52/125 [00:33<01:00,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 60.88 | optimizer_step: 403.49 | optimizer_allgather: 4.87\n",
            " 42% 53/125 [00:34<00:59,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 68.38 | optimizer_step: 406.97 | optimizer_allgather: 4.87\n",
            " 43% 54/125 [00:35<00:58,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 61.92 | optimizer_step: 404.24 | optimizer_allgather: 4.89\n",
            " 44% 55/125 [00:35<00:57,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 60.12 | optimizer_step: 398.95 | optimizer_allgather: 4.86\n",
            " 45% 56/125 [00:36<00:56,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 60.40 | optimizer_step: 404.35 | optimizer_allgather: 4.87\n",
            " 46% 57/125 [00:37<00:56,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 60.69 | optimizer_step: 397.10 | optimizer_allgather: 4.88\n",
            " 46% 58/125 [00:38<00:55,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 61.62 | optimizer_step: 404.06 | optimizer_allgather: 4.87\n",
            " 47% 59/125 [00:39<00:54,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 59.66 | optimizer_step: 399.92 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:33:41,229] [INFO] [logging.py:60:log_dist] [Rank 0] step=60, skipped=30, lr=[1.641872179772209e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:33:41,229] [INFO] [timer.py:166:stop] 0/60, SamplesPerSec=27.378275032419044\n",
            " 48% 60/125 [00:40<00:54,  1.20it/s][2021-03-04 16:33:41,584] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8.0, reducing to 4.0\n",
            " 49% 61/125 [00:40<00:44,  1.45it/s]rank=0 time (ms) | optimizer_gradients: 60.72 | optimizer_step: 404.27 | optimizer_allgather: 4.88\n",
            " 50% 62/125 [00:41<00:45,  1.40it/s]rank=0 time (ms) | optimizer_gradients: 61.81 | optimizer_step: 402.87 | optimizer_allgather: 4.88\n",
            " 50% 63/125 [00:41<00:45,  1.36it/s]rank=0 time (ms) | optimizer_gradients: 62.25 | optimizer_step: 403.23 | optimizer_allgather: 4.87\n",
            " 51% 64/125 [00:42<00:46,  1.32it/s]rank=0 time (ms) | optimizer_gradients: 61.37 | optimizer_step: 399.62 | optimizer_allgather: 4.89\n",
            " 52% 65/125 [00:43<00:45,  1.31it/s]rank=0 time (ms) | optimizer_gradients: 60.57 | optimizer_step: 405.90 | optimizer_allgather: 4.88\n",
            " 53% 66/125 [00:44<00:45,  1.31it/s]rank=0 time (ms) | optimizer_gradients: 61.08 | optimizer_step: 400.01 | optimizer_allgather: 4.87\n",
            " 54% 67/125 [00:45<00:44,  1.29it/s]rank=0 time (ms) | optimizer_gradients: 61.38 | optimizer_step: 402.63 | optimizer_allgather: 4.93\n",
            " 54% 68/125 [00:45<00:44,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 60.42 | optimizer_step: 403.20 | optimizer_allgather: 4.88\n",
            " 55% 69/125 [00:46<00:43,  1.28it/s]rank=0 time (ms) | optimizer_gradients: 60.49 | optimizer_step: 403.00 | optimizer_allgather: 4.91\n",
            "[2021-03-04 16:33:48,688] [INFO] [logging.py:60:log_dist] [Rank 0] step=70, skipped=31, lr=[1.7685242197620365e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:33:48,689] [INFO] [timer.py:166:stop] 0/70, SamplesPerSec=26.362932133757674\n",
            " 56% 70/125 [00:47<00:43,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 61.08 | optimizer_step: 399.81 | optimizer_allgather: 4.88\n",
            " 57% 71/125 [00:48<00:42,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 62.66 | optimizer_step: 405.52 | optimizer_allgather: 4.87\n",
            " 58% 72/125 [00:49<00:41,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 62.52 | optimizer_step: 399.48 | optimizer_allgather: 4.87\n",
            " 58% 73/125 [00:49<00:41,  1.26it/s]rank=0 time (ms) | optimizer_gradients: 60.26 | optimizer_step: 400.80 | optimizer_allgather: 4.88\n",
            " 59% 74/125 [00:50<00:41,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.26 | optimizer_step: 399.35 | optimizer_allgather: 4.87\n",
            " 60% 75/125 [00:51<00:40,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.33 | optimizer_step: 401.31 | optimizer_allgather: 4.87\n",
            " 61% 76/125 [00:52<00:39,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 62.84 | optimizer_step: 400.84 | optimizer_allgather: 4.87\n",
            " 62% 77/125 [00:53<00:38,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 63.95 | optimizer_step: 407.50 | optimizer_allgather: 4.90\n",
            " 62% 78/125 [00:53<00:37,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 61.78 | optimizer_step: 402.74 | optimizer_allgather: 4.89\n",
            " 63% 79/125 [00:54<00:36,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 62.10 | optimizer_step: 402.53 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:33:56,724] [INFO] [logging.py:60:log_dist] [Rank 0] step=80, skipped=31, lr=[1.8787123354240354e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:33:56,725] [INFO] [timer.py:166:stop] 0/80, SamplesPerSec=25.356222165572486\n",
            " 64% 80/125 [00:55<00:36,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 61.62 | optimizer_step: 402.67 | optimizer_allgather: 4.88\n",
            " 65% 81/125 [00:56<00:35,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 62.64 | optimizer_step: 408.45 | optimizer_allgather: 4.88\n",
            " 66% 82/125 [00:57<00:35,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 67.72 | optimizer_step: 408.33 | optimizer_allgather: 4.88\n",
            " 66% 83/125 [00:57<00:33,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.64 | optimizer_step: 402.85 | optimizer_allgather: 4.88\n",
            " 67% 84/125 [00:58<00:32,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 61.17 | optimizer_step: 400.94 | optimizer_allgather: 4.88\n",
            " 68% 85/125 [00:59<00:31,  1.26it/s]rank=0 time (ms) | optimizer_gradients: 63.93 | optimizer_step: 401.55 | optimizer_allgather: 4.89\n",
            " 69% 86/125 [01:00<00:31,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 61.86 | optimizer_step: 401.12 | optimizer_allgather: 4.86\n",
            " 70% 87/125 [01:01<00:30,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.90 | optimizer_step: 405.44 | optimizer_allgather: 4.91\n",
            " 70% 88/125 [01:01<00:29,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.94 | optimizer_step: 402.16 | optimizer_allgather: 4.86\n",
            " 71% 89/125 [01:02<00:28,  1.26it/s]rank=0 time (ms) | optimizer_gradients: 61.09 | optimizer_step: 400.58 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:34:04,717] [INFO] [logging.py:60:log_dist] [Rank 0] step=90, skipped=31, lr=[1.9683642376134485e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:34:04,718] [INFO] [timer.py:166:stop] 0/90, SamplesPerSec=24.648009273152017\n",
            " 72% 90/125 [01:03<00:27,  1.26it/s]rank=0 time (ms) | optimizer_gradients: 60.88 | optimizer_step: 400.93 | optimizer_allgather: 4.87\n",
            " 73% 91/125 [01:04<00:27,  1.26it/s]rank=0 time (ms) | optimizer_gradients: 60.37 | optimizer_step: 405.66 | optimizer_allgather: 4.87\n",
            " 74% 92/125 [01:05<00:26,  1.27it/s]rank=0 time (ms) | optimizer_gradients: 61.23 | optimizer_step: 399.52 | optimizer_allgather: 4.88\n",
            " 74% 93/125 [01:05<00:25,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 61.71 | optimizer_step: 402.51 | optimizer_allgather: 4.87\n",
            " 75% 94/125 [01:06<00:25,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.39 | optimizer_step: 400.80 | optimizer_allgather: 4.89\n",
            " 76% 95/125 [01:07<00:24,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 60.86 | optimizer_step: 401.34 | optimizer_allgather: 4.87\n",
            " 77% 96/125 [01:08<00:23,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 61.26 | optimizer_step: 400.68 | optimizer_allgather: 4.87\n",
            " 78% 97/125 [01:09<00:22,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 62.52 | optimizer_step: 403.54 | optimizer_allgather: 4.89\n",
            " 78% 98/125 [01:10<00:22,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 59.79 | optimizer_step: 400.11 | optimizer_allgather: 4.87\n",
            " 79% 99/125 [01:10<00:21,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 59.93 | optimizer_step: 401.60 | optimizer_allgather: 4.88\n",
            "[2021-03-04 16:34:12,802] [INFO] [logging.py:60:log_dist] [Rank 0] step=100, skipped=31, lr=[2.043945380404073e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:34:12,802] [INFO] [timer.py:166:stop] 0/100, SamplesPerSec=24.078023080729466\n",
            " 80% 100/125 [01:11<00:19,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 61.28 | optimizer_step: 398.15 | optimizer_allgather: 4.89\n",
            " 81% 101/125 [01:12<00:19,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 59.92 | optimizer_step: 405.00 | optimizer_allgather: 4.87\n",
            " 82% 102/125 [01:13<00:18,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 60.30 | optimizer_step: 399.51 | optimizer_allgather: 4.88\n",
            " 82% 103/125 [01:14<00:18,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 62.55 | optimizer_step: 405.25 | optimizer_allgather: 4.86\n",
            " 83% 104/125 [01:14<00:17,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 60.07 | optimizer_step: 395.99 | optimizer_allgather: 4.90\n",
            " 84% 105/125 [01:15<00:16,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 62.21 | optimizer_step: 403.65 | optimizer_allgather: 4.87\n",
            " 85% 106/125 [01:16<00:15,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 61.88 | optimizer_step: 400.69 | optimizer_allgather: 4.87\n",
            " 86% 107/125 [01:17<00:14,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 59.92 | optimizer_step: 401.67 | optimizer_allgather: 4.87\n",
            " 86% 108/125 [01:18<00:13,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 60.68 | optimizer_step: 399.55 | optimizer_allgather: 4.87\n",
            " 87% 109/125 [01:18<00:13,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 60.83 | optimizer_step: 401.52 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:34:21,014] [INFO] [logging.py:60:log_dist] [Rank 0] step=110, skipped=31, lr=[2.10927919344248e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:34:21,014] [INFO] [timer.py:166:stop] 0/110, SamplesPerSec=23.591614771106947\n",
            " 88% 110/125 [01:19<00:12,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 63.31 | optimizer_step: 400.20 | optimizer_allgather: 4.86\n",
            " 89% 111/125 [01:20<00:11,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 60.51 | optimizer_step: 401.07 | optimizer_allgather: 4.87\n",
            " 90% 112/125 [01:21<00:10,  1.20it/s]rank=0 time (ms) | optimizer_gradients: 61.05 | optimizer_step: 399.64 | optimizer_allgather: 4.87\n",
            " 90% 113/125 [01:22<00:09,  1.20it/s]rank=0 time (ms) | optimizer_gradients: 59.92 | optimizer_step: 402.70 | optimizer_allgather: 4.87\n",
            " 91% 114/125 [01:23<00:09,  1.21it/s]rank=0 time (ms) | optimizer_gradients: 61.14 | optimizer_step: 399.50 | optimizer_allgather: 4.88\n",
            " 92% 115/125 [01:23<00:08,  1.22it/s]rank=0 time (ms) | optimizer_gradients: 62.03 | optimizer_step: 404.13 | optimizer_allgather: 4.87\n",
            " 93% 116/125 [01:24<00:07,  1.23it/s]rank=0 time (ms) | optimizer_gradients: 60.89 | optimizer_step: 399.34 | optimizer_allgather: 4.88\n",
            " 94% 117/125 [01:25<00:06,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 61.10 | optimizer_step: 400.53 | optimizer_allgather: 4.88\n",
            " 94% 118/125 [01:26<00:05,  1.24it/s]rank=0 time (ms) | optimizer_gradients: 61.27 | optimizer_step: 399.11 | optimizer_allgather: 4.87\n",
            " 95% 119/125 [01:27<00:04,  1.25it/s]rank=0 time (ms) | optimizer_gradients: 59.90 | optimizer_step: 404.89 | optimizer_allgather: 4.87\n",
            "[2021-03-04 16:34:29,094] [INFO] [logging.py:60:log_dist] [Rank 0] step=120, skipped=31, lr=[2.1668154927766464e-05], mom=[[0.9, 0.999]]\n",
            "[2021-03-04 16:34:29,094] [INFO] [timer.py:166:stop] 0/120, SamplesPerSec=23.23977735337808\n",
            " 96% 120/125 [01:27<00:03,  1.26it/s]rank=0 time (ms) | optimizer_gradients: 61.19 | optimizer_step: 402.55 | optimizer_allgather: 4.89\n",
            " 97% 121/125 [01:28<00:03,  1.26it/s][2021-03-04 16:34:30,255] [INFO] [stage2.py:1361:step] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4.0, reducing to 2.0\n",
            " 98% 122/125 [01:29<00:01,  1.51it/s]rank=0 time (ms) | optimizer_gradients: 61.46 | optimizer_step: 401.35 | optimizer_allgather: 4.88\n",
            " 98% 123/125 [01:29<00:01,  1.41it/s]rank=0 time (ms) | optimizer_gradients: 60.53 | optimizer_step: 401.90 | optimizer_allgather: 4.89\n",
            " 99% 124/125 [01:30<00:00,  1.35it/s]rank=0 time (ms) | optimizer_gradients: 60.89 | optimizer_step: 402.82 | optimizer_allgather: 4.86\n",
            "100% 125/125 [01:31<00:00,  1.29it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 94.461, 'train_samples_per_second': 1.323, 'epoch': 1.0}\n",
            "100% 125/125 [01:31<00:00,  1.37it/s]\n",
            "Saving model checkpoint to output_dir\n",
            "Configuration saved in output_dir/config.json\n",
            "Model weights saved in output_dir/pytorch_model.bin\n",
            "tokenizer config file saved in output_dir/tokenizer_config.json\n",
            "Special tokens file saved in output_dir/special_tokens_map.json\n",
            "Copy vocab file to output_dir/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                      =    1.0\n",
            "  init_mem_cpu_alloc_delta   =    2MB\n",
            "  init_mem_cpu_peaked_delta  =    0MB\n",
            "  init_mem_gpu_alloc_delta   =    0MB\n",
            "  init_mem_gpu_peaked_delta  =    0MB\n",
            "  train_mem_cpu_alloc_delta  =   79MB\n",
            "  train_mem_cpu_peaked_delta =    0MB\n",
            "  train_mem_gpu_alloc_delta  =  816MB\n",
            "  train_mem_gpu_peaked_delta = 6553MB\n",
            "  train_runtime              = 94.461\n",
            "  train_samples              =   2000\n",
            "  train_samples_per_second   =  1.323\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}